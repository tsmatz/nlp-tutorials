{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention (Machine Translation Example)\n",
    "\n",
    "In the [previous example](./07_encoder_decoder.ipynb), we saw sequence-to-sequence encoder-decoder architecture in machine translation.<br>\n",
    "In the previous example, the input sequence is encoded into a single context, and this context is used for decoding in all units in generated tokens.\n",
    "\n",
    "This architecture will not be flexible, and also not scalable. For instance, in case of machine translation, it will be difficult to translate a long text (such as, translate multiple sentences at once) unlike human translation. (Because a single context will not be enough to represent entire text, when the text is so long.)\n",
    "\n",
    "By introducing attention architecture, this constraint can be relaxed.<br>\n",
    "The attention is more elaborative and widely used architecture in today's NLP, and a lot of tasks (such as, machine translation, smart reply, etc) are researched by adding attention mechanism and worked well today.\n",
    "\n",
    "The overview outline of attention architecture is shown as follows.<br>\n",
    "In this network, the context ```c``` is computed and obtained in attention layer (```attend``` in the following diagram) on decoder, and the different context is then used in each units in the sequence for decoding. (In the following diagram, each ```attend``` layer is the same network and then shares the weight's parameters.)\n",
    "\n",
    "![encoder-decoder with attention architecture](./images/encoder_decoder_attention.png)\n",
    "\n",
    "Within attention layer, it uses previous state and encoder's outputs (not only final output, but outputs in all units), and it generates $\\{ \\alpha_j^i \\}\\;(i=1,\\ldots,n)$, in which $\\sum_i \\alpha_j^i = 1$, with dense net (FCNet) and softmax activation, where $n$ is the number of encoder's outputs and $j$ is time step in sequence. (See the following diagram.)<br>\n",
    "To say in abstraction, $\\{ \\alpha_j^i \\}\\;(i=1,\\ldots,n)$ means an alignment's weight at j-th time step for each source sequence outputs, $o_1^{\\prime}, o_2^{\\prime}, \\ldots, o_n^{\\prime}$.<br>\n",
    "(This $\\{ \\alpha_j^i \\}\\;(i=1,\\ldots,n)$ is then called attention weights.)\n",
    "\n",
    "> Note : The softmax function is often used for normalizing outputs (sum to one) in neural networks. See [here](https://tsmatz.wordpress.com/2017/08/30/regression-in-machine-learning-math-for-beginners/) for softmax function.\n",
    "\n",
    "And it finally generates context $c_j$ at j-th time step by $ c_j = \\sum_i^n \\alpha_j^i \\cdot o_i^{\\prime} $.\n",
    "\n",
    "![soft attention architecture](./images/soft_attention.png)\n",
    "\n",
    "> Note : This architecture is called **soft attention**, which is the first attention introduced in the context of sequence-to-sequence generation. (See Bahdanau et al.)<br>\n",
    "> There exist a lot of variants in attention architecture. See the [next example](./09_transformer.ipynb) for famous scaled dot-product attention (and self-attention) in transformer.\n",
    "\n",
    "With this network, it can focus on specific components in source sequence.<br>\n",
    "For instance, in case of the following French-to-English machine translation, the 3rd units in sequence (\"don't\" in English) will strongly focus on 3rd and 5th components in original sequence (French), because the word \"don't\" will be strongly related to \"ne\" and \"pas\" in French. On the other hand, the components \"je\" and \"comprends\" in French are weakly referred, because it's not directly related to \"don't\" in English, but it's used only for determining not \"doesn't\" or not \"isn't\".<br>\n",
    "As a result, the attention weights $\\{ \\alpha_j^i \\}\\;(i=1,\\ldots,n)$ will be larger for the source components \"ne\" and \"pas\", and will be smaller for the source components \"je\" and \"comprends\".\n",
    "\n",
    "![attend in machine translation](./images/attend_image.png)\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/nlp-tutorials/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note : Currently torch 1.13.1 for cuda 11.4 has bugs (in which we can't run ```nn.Linear``` with ```out_features=1```) and we then use cuda 11.8 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1 torchtext==0.14.1 --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I use Engligh-French dataset by [Anki](https://www.manythings.org/anki/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-17 13:37:35--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6720195 (6.4M) [application/zip]\n",
      "Saving to: ‘fra-eng.zip’\n",
      "\n",
      "fra-eng.zip         100%[===================>]   6.41M   568KB/s    in 15s     \n",
      "\n",
      "2023-02-17 13:37:51 (429 KB/s) - ‘fra-eng.zip’ saved [6720195/6720195]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fra-eng.zip\n",
      "  inflating: fra-eng/_about.txt      \n",
      "  inflating: fra-eng/fra.txt         \n"
     ]
    }
   ],
   "source": [
    "!unzip fra-eng.zip -d fra-eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\r\n",
      "Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\r\n",
      "Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\r\n",
      "Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\r\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 fra-eng/fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197463 fra-eng/fra.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fra-eng/fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Va !', 'Go.'], dtype='<U349')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "pathobj = Path(\"fra-eng/fra.txt\")\n",
    "text_all = pathobj.read_text(encoding=\"utf-8\")\n",
    "lines = text_all.splitlines()\n",
    "train_data = [line.split(\"\\t\") for line in lines]\n",
    "train_data = np.array(train_data)[:,[1,0]]\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training set, text length in the latter part is longer (and includes multiple sentences) than the former part.<br>\n",
    "Therefore I shuffle entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dis toujours la vérité.', 'Always tell the truth.'], dtype='<U349')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(train_data)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data consists of multiple sentences, it converts to a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tsmatz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "tokenizer_en = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "tokenizer_fr = nltk.data.load(\"tokenizers/punkt/french.pickle\")\n",
    "fr_list = []\n",
    "en_list = []\n",
    "for x in train_data:\n",
    "    x1 = tokenizer_fr.tokenize(x[0])\n",
    "    x2 = tokenizer_en.tokenize(x[1])\n",
    "    if len(x1) == len(x2):\n",
    "        fr_list += x1\n",
    "        en_list += x2\n",
    "train_data = np.column_stack((fr_list, en_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), I standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation except \" ' \" (e.g, Ken's bag, ces't, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dis toujours la vérité', 'always tell the truth'], dtype='<U250')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "train_data = np.char.lower(train_data)\n",
    "train_data = np.char.replace(train_data, \"-\", \" \")\n",
    "for x in string.punctuation.replace(\"'\", \"\"):\n",
    "    train_data = np.char.replace(train_data, x, \"\")\n",
    "for x in \"«»\":\n",
    "    train_data = np.char.replace(train_data, x, \"\")\n",
    "train_data = np.char.strip(train_data)\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ```<start>``` and ```<end>``` tokens in string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<start> dis toujours la vérité <end>',\n",
       "       '<start> always tell the truth <end>'], dtype='<U264')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.array([[\" \".join([\"<start>\", x, \"<end>\"]), \" \".join([\"<start>\", y, \"<end>\"])] for x, y in train_data])\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequence inputs\n",
    "\n",
    "We will generate the sequence of word's indices (i.e, tokenize) from text.\n",
    "\n",
    "![Index vectorize](images/index_vectorize2.png?raw=true)\n",
    "\n",
    "First we create a list of vocabulary (```vocab```) for both source text (French) and target text (English) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "max_word = 10000\n",
    "\n",
    "# create space-split tokenizer\n",
    "tokenizer = get_tokenizer(None)\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        tokens = tokenizer(text)\n",
    "        yield tokens\n",
    "\n",
    "# build vocabulary list for French\n",
    "vocab_fr = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data[:,0]),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "vocab_fr.set_default_index(vocab_fr[\"<unk>\"])\n",
    "\n",
    "# build vocabulary list for English\n",
    "vocab_en = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data[:,1]),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated token index is ```0, 1, ... , vocab_size - 1```.<br>\n",
    "Now I set ```vocab_size``` as a token id in padded positions for both French and English respctively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index_fr = vocab_fr.__len__()\n",
    "vocab_fr.append_token(\"<pad>\")\n",
    "\n",
    "pad_index_en = vocab_en.__len__()\n",
    "vocab_en.append_token(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list for both index-to-word and word-to-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_fr = vocab_fr.get_itos()\n",
    "stoi_fr = vocab_fr.get_stoi()\n",
    "\n",
    "itos_en = vocab_en.get_itos()\n",
    "stoi_en = vocab_en.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of token index in French (source) is 10001.\n",
      "The padded index in French (source) is 10000.\n",
      "The number of token index in English (target) is 10001.\n",
      "The padded index in English (target) is 10000.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(\"The number of token index in French (source) is {}.\".format(vocab_fr.__len__()))\n",
    "print(\"The padded index in French (source) is {}.\".format(stoi_fr[\"<pad>\"]))\n",
    "print(\"The number of token index in English (target) is {}.\".format(vocab_en.__len__()))\n",
    "print(\"The padded index in English (target) is {}.\".format(stoi_en[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a collator function, which is used for pre-processing in data loader.\n",
    "\n",
    "In this collator,\n",
    "\n",
    "(1) First we create a list of word's indices for source (French) and target (English) respectively as follows.\n",
    "\n",
    "```<start> this is pen <end>``` --> ```[2, 7, 5, 14, 1]```\n",
    "\n",
    "(2) For target (English) sequence, we separate into features (x) and labels (y).<br>\n",
    "In this task, we predict the next word in target (English) sequence using the current word's sequence (English) and the encoded context of source (French).<br>\n",
    "We then separate target sequence into the sequence iteself (x) and the following label (y).\n",
    "\n",
    "<u>before</u> :\n",
    "\n",
    "```[2, 7, 5, 14, 1]```\n",
    "\n",
    "<u>after</u> :\n",
    "\n",
    "```x : [2, 7, 5, 14, 1]```\n",
    "\n",
    "```y : [7, 5, 14, 1, -100]```\n",
    "\n",
    "> Note : Here I set -100 as an unknown label id, because PyTorch cross-entropy function (```torch.nn.functional.cross_entropy()```) has a property ```ignore_index``` which default value is -100.\n",
    "\n",
    "(3) Finally we pad the inputs (for both source and target) as follows.<br>\n",
    "The padded index in features is ```pad_index``` and the padded index in label is -100. (See above note.)\n",
    "\n",
    "```x : [2, 7, 5, 14, 1, N, ... , N]```\n",
    "\n",
    "```y : [7, 5, 14, 1, -100, -100, ... , -100]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seq_len_fr = 45\n",
    "seq_len_en = 38\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, feature_source_list, feature_target_list = [], [], []\n",
    "    for text_fr, text_en in batch:\n",
    "        # (1) tokenize to a list of word's indices\n",
    "        tokens_fr = vocab_fr(tokenizer(text_fr))\n",
    "        tokens_en = vocab_en(tokenizer(text_en))\n",
    "        # (2) separate into features and labels in target tokens (English)\n",
    "        y = tokens_en[1:]\n",
    "        y.append(-100)\n",
    "        # (3) limit length to seq_len and pad sequence\n",
    "        y = y[:seq_len_en]\n",
    "        tokens_fr = tokens_fr[:seq_len_fr]\n",
    "        tokens_en = tokens_en[:seq_len_en]\n",
    "        y += [-100] * (seq_len_en - len(y))\n",
    "        tokens_fr += [pad_index_fr] * (seq_len_fr - len(tokens_fr))\n",
    "        tokens_en += [pad_index_en] * (seq_len_en - len(tokens_en))\n",
    "        # add to list\n",
    "        label_list.append(y)\n",
    "        feature_source_list.append(tokens_fr)\n",
    "        feature_target_list.append(tokens_en)\n",
    "    # convert to tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
    "    feature_source_list = torch.tensor(feature_source_list, dtype=torch.int64).to(device)\n",
    "    feature_target_list = torch.tensor(feature_target_list, dtype=torch.int64).to(device)\n",
    "    return label_list, feature_source_list, feature_target_list\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    list(zip(train_data[:,0], train_data[:,1])),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape in batch : torch.Size([64, 38])\n",
      "feature source shape in batch : torch.Size([64, 45])\n",
      "feature target shape in batch : torch.Size([64, 38])\n",
      "***** label sample *****\n",
      "tensor([   3,  450,  112,    1, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100], device='cuda:0')\n",
      "***** features (source) sample *****\n",
      "tensor([    2,    23,   624,    11,   103,     1, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000], device='cuda:0')\n",
      "***** features (target) sample *****\n",
      "tensor([    2,     3,   450,   112,     1, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for labels, sources, targets in dataloader:\n",
    "    break\n",
    "\n",
    "print(\"label shape in batch : {}\".format(labels.size()))\n",
    "print(\"feature source shape in batch : {}\".format(sources.size()))\n",
    "print(\"feature target shape in batch : {}\".format(targets.size()))\n",
    "print(\"***** label sample *****\")\n",
    "print(labels[0])\n",
    "print(\"***** features (source) sample *****\")\n",
    "print(sources[0])\n",
    "print(\"***** features (target) sample *****\")\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network\n",
    "\n",
    "Now we build an attention model in encoder-decoder architecture as follows.\n",
    "\n",
    "- Outputs (not only final output, but all outputs in all units) in RNN for source French text are generated in encoder.\n",
    "- Encoder's outputs are used in attention architecture and the result is passed into unit in decoder's RNN.\n",
    "- Each RNN output in decoder is passed into dense (FCNet) layer and generate the sequence of next words.\n",
    "- Calculate loss between predicted next words and the true values of next words, and then proceed to optimize neural networks.\n",
    "\n",
    "![the trainer architecture of machine translation](./images/machine_translation2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build encoder model.<br>\n",
    "See the [previous examples](./06_language_model_rnn.ipynb) for details about RNN inputs and outputs in PyTorch. (Here I also use packed sequence, because I want to process appropriate time-steps in each sequence.)\n",
    "\n",
    "Unlike [previous example](./07_encoder_decoder.ipynb) (vanilla encoder-decoder example), all outputs in all units are used in the decoder, and the encoder should then return all outputs (not only the final output).\n",
    "\n",
    "![all outputs in encoder](./images/encoder_all.png)\n",
    "\n",
    "I note that the size of the following ```masks``` output is ```(batch_size, seq_len)```, in which its element's value is 0 when it's a padded position, and otherwise 1. (This ```masks``` will then be used in the following softmax operation in decoder side.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embedding_dim, rnn_units, padding_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_units,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # embedding\n",
    "        #   --> (batch_size, seq_len, embedding_dim)\n",
    "        outs = self.embedding(inputs)\n",
    "        # build \"lengths\" property to pack inputs (see previous example)\n",
    "        masks = (inputs != self.padding_idx).int()\n",
    "        lengths = masks.sum(dim=1, keepdim=False)\n",
    "        # pack inputs for RNN (see previous example)\n",
    "        packed_inputs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            outs,\n",
    "            lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        # apply RNN\n",
    "        packed_outs, _ = self.rnn(packed_inputs)\n",
    "        # unpack results\n",
    "        #   --> (batch_size, seq_len, rnn_units)\n",
    "        outs, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outs,\n",
    "            batch_first=True,\n",
    "            padding_value=0.0,\n",
    "            total_length=self.seq_len,\n",
    "        )\n",
    "        return outs, masks\n",
    "\n",
    "enc_model = Encoder(\n",
    "    vocab_size=vocab_fr.__len__(),\n",
    "    seq_len=seq_len_fr,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    padding_idx=pad_index_fr).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build decoder with attention architecture as follows.\n",
    "\n",
    "![decoder with attention](./images/decoder_attention.png)\n",
    "\n",
    "In each time-steps in target sequence, the state in previous step is used in computation of attention layer, and it repeats this process until the end of sequence. (See the following ```for``` loop.)\n",
    "\n",
    "In each steps, first, the previous state and encoder's outputs are concatenated, and the results are passed into dense network (FCNet).<br>\n",
    "By applying softmax function for this output, the attention weights $\\alpha$ (```alpha``` in the following code) at ```j```-th step are obtained. The context ```c``` at ```j```-th step is then generated by $\\sum_i \\alpha_i o_i^{\\prime}$ where $o_i^{\\prime}$ is i-th element in encoder's outputs.<br>\n",
    "In the following code, the padded elements in the softmax operation will be ignored (masked), because $e^{-inf} = 0$.\n",
    "\n",
    "Once we get the context ```c```, the subsequent steps are the same as [previous example](./07_encoder_decoder.ipynb). (See previous example for details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, padding_idx, hidden_dim1=1024, hidden_dim2=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "        # Below are used in attention layer\n",
    "        self.attention_dense1 = nn.Linear(rnn_units*2, hidden_dim1)\n",
    "        self.attention_dense2 = nn.Linear(hidden_dim1, 1)\n",
    "\n",
    "        # Below are used in other parts\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.rnncell = nn.GRUCell(\n",
    "            input_size=rnn_units + embedding_dim,\n",
    "            hidden_size=rnn_units,\n",
    "        )\n",
    "        self.output_dense1 = nn.Linear(rnn_units, hidden_dim2)\n",
    "        self.output_dense2 = nn.Linear(hidden_dim2, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, enc_outputs, enc_masks, states=None, return_states=False):\n",
    "        #\n",
    "        # get size\n",
    "        #\n",
    "\n",
    "        batch_size = inputs.size()[0]\n",
    "        dec_seq_size = inputs.size()[1]\n",
    "        enc_seq_size = enc_outputs.size()[1]\n",
    "\n",
    "        #\n",
    "        # set initial states\n",
    "        #\n",
    "\n",
    "        if states is None:\n",
    "            current_states = torch.zeros((batch_size, self.rnn_units)).to(device)\n",
    "        else:\n",
    "            current_states = states\n",
    "\n",
    "        # loop target sequence\n",
    "        #   [Note] Here I loop in all time-steps, but please filter\n",
    "        #   for saving resource's consumption.\n",
    "        #   (Sort batch, run by filtering, and turn into original position.)\n",
    "        rnn_outputs = []\n",
    "        for j in range(dec_seq_size):\n",
    "\n",
    "            #\n",
    "            # process attention\n",
    "            #\n",
    "\n",
    "            #   --> (batch_size, 1, rnn_units)\n",
    "            current_states_reshaped = current_states.unsqueeze(dim=1)\n",
    "            #   --> (batch_size, enc_seq_size, rnn_units)\n",
    "            current_states_reshaped = current_states_reshaped.expand(-1, enc_seq_size, -1)\n",
    "            # concat\n",
    "            #   --> (batch_size, enc_seq_size, rnn_units * 2)\n",
    "            enc_and_states = torch.concat((current_states_reshaped, enc_outputs), dim=-1)\n",
    "            # apply dense\n",
    "            #   --> (batch_size, enc_seq_size, 1)\n",
    "            alpha = self.attention_dense1(enc_and_states)\n",
    "            alpha = F.relu(alpha)\n",
    "            alpha = self.attention_dense2(alpha)\n",
    "            #   --> (batch_size, enc_seq_size)\n",
    "            alpha = alpha.squeeze(dim=2)\n",
    "            # apply masked softmax\n",
    "            alpha = alpha.masked_fill(enc_masks == 0, float(\"-inf\"))\n",
    "            alpha = F.softmax(alpha, dim=-1)\n",
    "            # get context\n",
    "            #   --> (batch_size, rnn_units)\n",
    "            c = torch.einsum(\"bs,bsu->bu\", alpha, enc_outputs)\n",
    "\n",
    "            #\n",
    "            # process rnn\n",
    "            #\n",
    "\n",
    "            # embedding\n",
    "            #   --> (batch_size, embedding_dim)\n",
    "            emb_j = self.embedding(inputs[:,j])\n",
    "            # concat\n",
    "            #   --> (batch_size, rnn_units + embedding_dim)\n",
    "            input_j = torch.concat((c, emb_j), dim=-1)\n",
    "            # apply rnn (proceed to the next state)\n",
    "            current_states = self.rnncell(input_j, current_states)\n",
    "            # append state\n",
    "            rnn_outputs.append(current_states)\n",
    "\n",
    "        #\n",
    "        # process outputs\n",
    "        #\n",
    "\n",
    "        # get output state's tensor\n",
    "        #   --> (batch_size, dec_seq_size, rnn_units)\n",
    "        rnn_outputs = torch.stack(rnn_outputs, dim=1)\n",
    "        # apply dense\n",
    "        #   --> (batch_size, dec_seq_size, vocab_size)\n",
    "        outs = self.output_dense1(rnn_outputs)\n",
    "        outs = F.relu(outs)\n",
    "        logits = self.output_dense2(outs)\n",
    "\n",
    "        # return results\n",
    "        if return_states:\n",
    "            # set 0.0 in padded position\n",
    "            masks = (inputs != self.padding_idx).int()\n",
    "            masks = masks.unsqueeze(dim=2)\n",
    "            masks = masks.expand(-1, -1, self.rnn_units)\n",
    "            rnn_outputs = rnn_outputs.masked_fill(masks == 0, 0.0)\n",
    "            return logits, rnn_outputs  # This is used in prediction\n",
    "        else:\n",
    "            return logits               # This is used in training\n",
    "\n",
    "dec_model = DecoderWithAttention(\n",
    "    vocab_size=vocab_en.__len__(),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    padding_idx=pad_index_en).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Now we put it all together and run training.\n",
    "\n",
    "The loss on label id=-100 is ignored in ```cross_entropy()``` function. The padded position and the end of sequence will then be ignored in optimization.\n",
    "\n",
    "> Note : Because the default value of  ```ignore_index``` property in ```cross_entropy()``` function is -100. (You can change this default value.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 1.8736 - accuracy: 0.6900\n",
      "Epoch 2 - loss: 1.8001 - accuracy: 0.6667\n",
      "Epoch 3 - loss: 0.7754 - accuracy: 0.8667\n",
      "Epoch 4 - loss: 0.1816 - accuracy: 0.9286\n",
      "Epoch 5 - loss: 0.7531 - accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "all_params = list(enc_model.parameters()) + list(dec_model.parameters())\n",
    "optimizer = torch.optim.AdamW(all_params, lr=0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, sources, targets in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        enc_outputs, enc_masks = enc_model(sources)\n",
    "        logits = dec_model(targets, enc_outputs, enc_masks)\n",
    "        loss = F.cross_entropy(logits.transpose(1,2), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=2)\n",
    "        num_correct = (pred_labels == labels).float().sum()\n",
    "        num_total = (labels != -100).float().sum()\n",
    "        accuracy = num_correct / num_total\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Text\n",
    "\n",
    "Now translate French text to English text with trained model. (All these sentences are not in training set.)\n",
    "\n",
    "Here I simply translate several brief sentences, but the metrics to evaluate text-generation task will not be so easy. (Because simply checking an exact match to a reference text is not optimal.)<br>\n",
    "To eveluate the trained model, use some common metrics available in text generation, such as, BLEU or ROUGE.\n",
    "\n",
    "> Note : Here I use greedy search and this will sometimes lead to wrong sequence. For drawbacks and solutions, see note in [this example](./05_language_model_basic.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "end_index_en = stoi_en[\"<end>\"]\n",
    "max_output = 128\n",
    "\n",
    "def translate(sentence):\n",
    "    # preprocess inputs\n",
    "    text_fr = sentence\n",
    "    text_fr = text_fr.lower()\n",
    "    text_fr = \" \".join([\"<start>\", text_fr, \"<end>\"])\n",
    "    text_en = \"<start>\"\n",
    "    _, tokens_fr, tokens_en = collate_batch(list(zip([text_fr], [text_en])))\n",
    "\n",
    "    # process encoder\n",
    "    enc_outputs, enc_masks = enc_model(tokens_fr)\n",
    "\n",
    "    # process decoder\n",
    "    final_state = None\n",
    "    for loop in range(max_output):\n",
    "        logits, states = dec_model(\n",
    "            tokens_en,\n",
    "            enc_outputs,\n",
    "            enc_masks,\n",
    "            states=final_state,\n",
    "            return_states=True)\n",
    "        final_state = states[0][0].unsqueeze(dim=0)\n",
    "        pred_idx_en = logits[0][0].argmax()\n",
    "        next_word_en = itos_en[pred_idx_en]\n",
    "        text_en += \" \"\n",
    "        text_en += next_word_en\n",
    "        if pred_idx_en.item() == end_index_en:\n",
    "            break\n",
    "        _, _, tokens_en = collate_batch(list(zip([\"<end>\"], [next_word_en])))\n",
    "    return text_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i like the guitar <end>\n",
      "<start> he lives in japan <end>\n",
      "<start> this pen is used to him <end>\n",
      "<start> that's my favorite song <end>\n",
      "<start> he drives a car and goes to new york <end>\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"j'aime la guitare\")) # i like guitar\n",
    "print(translate(\"il vit au japon\")) # he lives in Japan\n",
    "print(translate(\"ce stylo est utilisé par lui\")) # this pen is used by him\n",
    "print(translate(\"c'est ma chanson préférée\")) # that's my favorite song\n",
    "print(translate(\"il conduit une voiture et va à new york\")) # he drives a car and goes to new york"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
