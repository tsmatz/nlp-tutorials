{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram detection with 1D Convolution\n",
    "\n",
    "In the previous examples, we have run tasks on embeddings in each word, but we sometimes should handle a set of ordered items.<br>\n",
    "For instance, \"hot dog\" won't be in the species of \"dog\", but it will be a part of food. \"Paris Hilton\" will also be far from \"Paris\" in language context. Even when you find \"good\" in the sentence, it might be a signal of negative sentiment in the context \"not good\".<br>\n",
    "Not only bi-grams, but the same is true for tri-grams and generic N-grams.\n",
    "\n",
    "The convolution network (CNN) is a today's widely used model in computer vision (such as, image classification, object detection, segmentation, etc). In NLP, this convolutional architecture can also be applied in N-gram detection.<br>\n",
    "In computer vision, 2D convolution (convolution by 2 dimensions of width and height) is generally used, but in N-gram detection, 1D convolution is applied as follows.\n",
    "\n",
    "![Bi-gram CNN](images/bigram_convolution.png?raw=true)\n",
    "\n",
    "There exist several variations for N-gram detection by convolutions.<br>\n",
    "The hierarchical convolutions can capture patterns with gaps, such as, \"not --- good\" or \"see --- little\" where \"---\" stands for a short sequence of words.<br>\n",
    "Similar to image processing, multiple channels can also be applied in NLP convolution. For instance, when each word has multiple embeddings (such as, word embedding, POS-tag embedding, position-wise word embedding, etc), these embeddings can be manipulated as multiple channels in NLP. Or, after applying multiple N-grams (such as, 2-gram, 4-gram, and 6-gram), the results can also be manipulated as multiple channels.\n",
    "\n",
    "In this example, for the purpose of your beginning, I'll simply apply bi-gram detection using 1D convolution with a single channel.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/nlp-tutorials/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.6.2 pandas numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in [previous example](./03_word2vec.ipynb), here I also use text in news papers dataset. (However, in this example, we use 2 columns of \"headline\" and \"short description\".)\n",
    "\n",
    "Before starting, please download [News_Category_Dataset_v2.json](https://www.kaggle.com/datasets/rmisra/news-category-dataset) (collected by HuffPost) in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"News_Category_Dataset_v2.json\",lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll apply text classification task.\n",
    "\n",
    "If we handle a long text, words appearing early will be more indicative (topical) rather than others. In practical text classification, a long text will then be separated into **regions**. In each region, the convolution (with pooling) is applied and then concatenated. (See below.)<br>\n",
    "For instance, with RCV1 (Reuters Corpus Volume I) dataset, 20 equally sized regions has better performance in category classification. (See [Johnson and Zhang (2015)](https://arxiv.org/abs/1504.01255).)\n",
    "\n",
    "![region separation](images/region_separation.png?raw=true)\n",
    "\n",
    "In this example, ```headline``` and ```short_description``` are both short text, and we then treat these features as regions, instead of separating into regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "0       There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1       Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2         Hugh Grant Marries For The First Time At Age 57   \n",
       "3       Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4       Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "...                                                   ...   \n",
       "200848  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "200849  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "200850  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "200851  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "200852  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                                        short_description  \n",
       "0       She left her husband. He killed their children...  \n",
       "1                                Of course it has a song.  \n",
       "2       The actor and his longtime girlfriend Anna Ebe...  \n",
       "3       The actor gives Dems an ass-kicking for not fi...  \n",
       "4       The \"Dietland\" actress said using the bags is ...  \n",
       "...                                                   ...  \n",
       "200848  Verizon Wireless and AT&T are already promotin...  \n",
       "200849  Afterward, Azarenka, more effusive with the pr...  \n",
       "200850  Leading up to Super Bowl XLVI, the most talked...  \n",
       "200851  CORRECTION: An earlier version of this story i...  \n",
       "200852  The five-time all-star center tore into his te...  \n",
       "\n",
       "[200853 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data[[\"headline\", \"short_description\"]]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), we standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation\n",
    "\n",
    "> Note : Lemmatization (standardization for such as \"have\", \"had\" or \"having\") should be dealed with, but here I have skipped these pre-processing.<br>\n",
    "> In the strict pre-processing, we should also care about the polysemy. (The different meanings in the same word should have different tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "# to lowercase\n",
    "train_data = train_data.apply(lambda x: x.str.lower())\n",
    "\n",
    "# replace hyphen\n",
    "train_data = train_data.apply(lambda x: x.str.replace(\"-\",\" \"))\n",
    "\n",
    "# remove stop words (only when it includes punctuation)\n",
    "for w in stopwords.words(\"english\"):\n",
    "    if re.match(\"(^|\\w+)[%s](\\w+|$)\" % re.escape(string.punctuation), w):\n",
    "        train_data = train_data.apply(lambda x: x.str.replace(\"(^|\\s+)%s(\\s+|$)\" % re.escape(w),\" \",regex=True))\n",
    "train_data = train_data.apply(lambda x: x.str.strip())\n",
    "\n",
    "# remove punctuation\n",
    "train_data = train_data.apply(lambda x: x.str.replace(\"[%s]\" % re.escape(string.punctuation),\"\",regex=True))\n",
    "train_data = train_data.apply(lambda x: x.str.strip())\n",
    "\n",
    "# remove stop words (only when it doesn't include punctuation)\n",
    "for w in stopwords.words(\"english\"):\n",
    "    if not re.match(\"(^|\\w+)[%s](\\w+|$)\" % re.escape(string.punctuation), w):\n",
    "        train_data = train_data.apply(lambda x: x.str.replace(\"(^|\\s+)%s(\\s+|$)\" % re.escape(w),\" \",regex=True))\n",
    "train_data = train_data.apply(lambda x: x.str.strip())\n",
    "\n",
    "# drop Nan\n",
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 mass shootings texas last week 1 tv</td>\n",
       "      <td>left husband killed children another day america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smith joins diplo nicky jam 2018 world cups of...</td>\n",
       "      <td>course song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hugh grant marries first time age 57</td>\n",
       "      <td>actor longtime girlfriend anna eberstein tied ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim carrey blasts castrato adam schiff democra...</td>\n",
       "      <td>actor gives dems ass kicking fighting hard eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>julianna margulies uses donald trump poop bags...</td>\n",
       "      <td>dietland actress said using bags really cathar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>rim ceo thorsten heins significant plans black...</td>\n",
       "      <td>verizon wireless att already promoting lte dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>maria sharapova stunned victoria azarenka aust...</td>\n",
       "      <td>afterward azarenka effusive press normal credi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>giants patriots jets colts among improbable su...</td>\n",
       "      <td>leading super bowl xlvi talked game could end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>aldon smith arrested 49ers linebacker busted dui</td>\n",
       "      <td>correction earlier version story incorrectly s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>dwight howard rips teammates magic loss hornets</td>\n",
       "      <td>five time star center tore teammates friday ni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "0                   2 mass shootings texas last week 1 tv   \n",
       "1       smith joins diplo nicky jam 2018 world cups of...   \n",
       "2                    hugh grant marries first time age 57   \n",
       "3       jim carrey blasts castrato adam schiff democra...   \n",
       "4       julianna margulies uses donald trump poop bags...   \n",
       "...                                                   ...   \n",
       "200848  rim ceo thorsten heins significant plans black...   \n",
       "200849  maria sharapova stunned victoria azarenka aust...   \n",
       "200850  giants patriots jets colts among improbable su...   \n",
       "200851   aldon smith arrested 49ers linebacker busted dui   \n",
       "200852    dwight howard rips teammates magic loss hornets   \n",
       "\n",
       "                                        short_description  \n",
       "0        left husband killed children another day america  \n",
       "1                                             course song  \n",
       "2       actor longtime girlfriend anna eberstein tied ...  \n",
       "3       actor gives dems ass kicking fighting hard eno...  \n",
       "4       dietland actress said using bags really cathar...  \n",
       "...                                                   ...  \n",
       "200848  verizon wireless att already promoting lte dev...  \n",
       "200849  afterward azarenka effusive press normal credi...  \n",
       "200850  leading super bowl xlvi talked game could end ...  \n",
       "200851  correction earlier version story incorrectly s...  \n",
       "200852  five time star center tore teammates friday ni...  \n",
       "\n",
       "[200853 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in previous examples, we will generate a vectorizer, which converts each text to the sequence of word's indices (a vector with 140 dimensions) as follows.<br>\n",
    "When the length of text is smaller than 140, the vector is padded by zero.\n",
    "\n",
    "![Index vectorize](images/index_vectorize.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 50000\n",
    "max_seq_len = 140\n",
    "\n",
    "# Set up vectorizer\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_sequence_length=max_seq_len,\n",
    "    output_mode=\"int\",\n",
    "    pad_to_max_tokens=False,\n",
    "    trainable=False)\n",
    "\n",
    "# concat columns (headline and short_description)\n",
    "text_all = pd.concat([train_data[\"headline\"], train_data[\"short_description\"]])\n",
    "\n",
    "# create vocabulary list\n",
    "# (UNK is automatically included)\n",
    "vectorizer.adapt(text_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build network.<br>\n",
    "In this neural network,\n",
    "\n",
    "1. As you saw in previous examples, we build embedding vectors (dense vectors) $ \\{ \\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_m \\} $ from text for both ```headline``` and ```short_description``` respectively.\n",
    "2. For these embedding vectors, we apply 1D convolution $ \\mathbf{p}_i = g(U (\\mathbf{x}_i) + \\mathbf{b}) $ where $ \\mathbf{x}_i = [\\mathbf{w}_i, \\mathbf{w}_{i+1}] $, $U$ is a weight matrix, $\\mathbf{b}$ is a bias vector, and $ g() $ is RELU activaiton. (i.e, In convolutions, the size of window is 2 (bi-gram) and the size of stride is 1.)<br>\n",
    "In this example, we apply half padding convolution (i.e, apply $ \\mathbf{x}_i = [\\mathbf{w}_i, \\mathbf{w}_{i+1}] $ for $ i=1,\\ldots,m $ where $\\mathbf{w}_{m+1}$ is zero) and the number of outputs will then also be $m$.<br>\n",
    "I assume that the result is $n$-dimensional vectors $ \\mathbf{p}_1, \\mathbf{p}_2, \\cdots, \\mathbf{p}_m $ .\n",
    "3. Next we apply $\\mathbf{c}_{[j]} = \\max_{1 \\leq i \\leq m} \\mathbf{p}_{i [j]} \\forall j \\in [1,n]$ and get $n$-dimensional vector $\\mathbf{c}$. (i.e, max pooling)<br>\n",
    "Here I have denoted $j$-th element of vecotr $\\mathbf{p}_i$ by $\\mathbf{p}_{i [j]}$. ($i \\in [1,m], j \\in [1,n]$)\n",
    "4. We concatenate the result's vectors $\\mathbf{c}$ and $\\mathbf{d}$, each of which is corresponing to ```headline``` and ```short_description```.\n",
    "5. Finally, we apply fully-connected feed-forward network (i.e, Dense Net) for predicting one-hot class value.\n",
    "\n",
    "![composing network](images/1d_conv_net.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramClassificationModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(BigramClassificationModel, self).__init__()\n",
    "\n",
    "        #\n",
    "        # input definition (shape : (batch_size, 1, ))\n",
    "        #\n",
    "\n",
    "        input1 = tf.keras.layers.Input(\n",
    "            dtype=tf.string,\n",
    "            shape=(1, ),\n",
    "            name=\"text1\")\n",
    "        input2 = tf.keras.layers.Input(\n",
    "            dtype=tf.string,\n",
    "            shape=(1, ),\n",
    "            name=\"text2\")\n",
    "\n",
    "        #\n",
    "        # Apply convolution for input1\n",
    "        #\n",
    "\n",
    "        # vectorize (shape : (batch_size, 140))\n",
    "        vec1 = vectorizer(input1)\n",
    "        # word's embedding (shape : (batch_size, 140, 200))\n",
    "        emb1 = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            trainable=True,\n",
    "            name=\"embedding1\")(vec1)\n",
    "        # apply convolution (shape : (batch_size, 140, 256) - because the number of bi-gram segments is same as word's count.)\n",
    "        conv1 = tf.keras.layers.Conv1D(\n",
    "            256,\n",
    "            2,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            trainable=True)(emb1)\n",
    "        # apply maxpool (shape : (batch_size, 1, 256)\n",
    "        pool1 = tf.keras.layers.MaxPool1D(pool_size=max_seq_len)(conv1)\n",
    "        # reshape : (batch_size, 256)\n",
    "        flat1 = tf.keras.layers.Flatten()(pool1)\n",
    "\n",
    "        #\n",
    "        # Apply convolution for input2\n",
    "        #\n",
    "\n",
    "        # vectorize (shape : (batch_size, 140))\n",
    "        vec2 = vectorizer(input2)\n",
    "        # word's embedding (shape : (batch_size, 140, 200))\n",
    "        emb2 = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            trainable=True,\n",
    "            name=\"embedding2\")(vec2)\n",
    "        # apply convolution (shape : (batch_size, 140, 256) - because the number of bi-gram segments is same as word's count.)\n",
    "        conv2 = tf.keras.layers.Conv1D(\n",
    "            256,\n",
    "            2,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            trainable=True,)(emb2)\n",
    "        # apply maxpool (shape : (batch_size, 1, 256)\n",
    "        pool2 = tf.keras.layers.MaxPool1D(pool_size=max_seq_len)(conv2)\n",
    "        # reshape : (batch_size, 256)\n",
    "        flat2 = tf.keras.layers.Flatten()(pool2)\n",
    "\n",
    "        #\n",
    "        # concatenate each pool (shape : (batch_size, 512))\n",
    "        #\n",
    "\n",
    "        news_feature = tf.keras.layers.Concatenate(axis=-1)(\n",
    "            [flat1, flat2])\n",
    "\n",
    "        #\n",
    "        # classify by fully-connected feed forward network\n",
    "        #\n",
    "\n",
    "        hidden1 = tf.keras.layers.Dense(\n",
    "            128,\n",
    "            activation=\"relu\",\n",
    "            trainable=True)(news_feature)\n",
    "        outputs = tf.keras.layers.Dense(\n",
    "            41,\n",
    "            activation=None,\n",
    "            trainable=True)(hidden1)\n",
    "\n",
    "        #\n",
    "        # Generate model\n",
    "        #\n",
    "\n",
    "        self.base_model = tf.keras.Model(\n",
    "            inputs=[input1, input2],\n",
    "            outputs=outputs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input1, input2 = inputs\n",
    "        return self.base_model({\n",
    "            \"text1\": input1,\n",
    "            \"text2\": input2\n",
    "        })\n",
    "\n",
    "#\n",
    "# define and generate model\n",
    "#\n",
    "embedding_dim = 200\n",
    "model = BigramClassificationModel(vocab_size, embedding_dim)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ```category``` column for label data in training.<br>\n",
    "The following output is one-hot label values in each row of dataset. (It has 41 categories.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTS</th>\n",
       "      <th>ARTS &amp; CULTURE</th>\n",
       "      <th>BLACK VOICES</th>\n",
       "      <th>BUSINESS</th>\n",
       "      <th>COLLEGE</th>\n",
       "      <th>COMEDY</th>\n",
       "      <th>CRIME</th>\n",
       "      <th>CULTURE &amp; ARTS</th>\n",
       "      <th>DIVORCE</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>...</th>\n",
       "      <th>TASTE</th>\n",
       "      <th>TECH</th>\n",
       "      <th>THE WORLDPOST</th>\n",
       "      <th>TRAVEL</th>\n",
       "      <th>WEDDINGS</th>\n",
       "      <th>WEIRD NEWS</th>\n",
       "      <th>WELLNESS</th>\n",
       "      <th>WOMEN</th>\n",
       "      <th>WORLD NEWS</th>\n",
       "      <th>WORLDPOST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ARTS  ARTS & CULTURE  BLACK VOICES  BUSINESS  COLLEGE  COMEDY  CRIME  \\\n",
       "0          0               0             0         0        0       0      1   \n",
       "1          0               0             0         0        0       0      0   \n",
       "2          0               0             0         0        0       0      0   \n",
       "3          0               0             0         0        0       0      0   \n",
       "4          0               0             0         0        0       0      0   \n",
       "...      ...             ...           ...       ...      ...     ...    ...   \n",
       "200848     0               0             0         0        0       0      0   \n",
       "200849     0               0             0         0        0       0      0   \n",
       "200850     0               0             0         0        0       0      0   \n",
       "200851     0               0             0         0        0       0      0   \n",
       "200852     0               0             0         0        0       0      0   \n",
       "\n",
       "        CULTURE & ARTS  DIVORCE  EDUCATION  ...  TASTE  TECH  THE WORLDPOST  \\\n",
       "0                    0        0          0  ...      0     0              0   \n",
       "1                    0        0          0  ...      0     0              0   \n",
       "2                    0        0          0  ...      0     0              0   \n",
       "3                    0        0          0  ...      0     0              0   \n",
       "4                    0        0          0  ...      0     0              0   \n",
       "...                ...      ...        ...  ...    ...   ...            ...   \n",
       "200848               0        0          0  ...      0     1              0   \n",
       "200849               0        0          0  ...      0     0              0   \n",
       "200850               0        0          0  ...      0     0              0   \n",
       "200851               0        0          0  ...      0     0              0   \n",
       "200852               0        0          0  ...      0     0              0   \n",
       "\n",
       "        TRAVEL  WEDDINGS  WEIRD NEWS  WELLNESS  WOMEN  WORLD NEWS  WORLDPOST  \n",
       "0            0         0           0         0      0           0          0  \n",
       "1            0         0           0         0      0           0          0  \n",
       "2            0         0           0         0      0           0          0  \n",
       "3            0         0           0         0      0           0          0  \n",
       "4            0         0           0         0      0           0          0  \n",
       "...        ...       ...         ...       ...    ...         ...        ...  \n",
       "200848       0         0           0         0      0           0          0  \n",
       "200849       0         0           0         0      0           0          0  \n",
       "200850       0         0           0         0      0           0          0  \n",
       "200851       0         0           0         0      0           0          0  \n",
       "200852       0         0           0         0      0           0          0  \n",
       "\n",
       "[200853 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.get_dummies(data[\"category\"])\n",
    "y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate TensorFlow dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one-hot list\n",
    "y = y_df.values.tolist()\n",
    "\n",
    "# generate tensorflow dataset (X, y)\n",
    "train_tf_data = tf.data.Dataset.from_tensor_slices((\n",
    "    (train_data[\"headline\"], train_data[\"short_description\"]),\n",
    "    y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "393/393 [==============================] - 134s 289ms/step - loss: 2.0092 - accuracy: 0.4895\n",
      "Epoch 2/15\n",
      "393/393 [==============================] - 115s 292ms/step - loss: 1.2020 - accuracy: 0.6859\n",
      "Epoch 3/15\n",
      "393/393 [==============================] - 115s 292ms/step - loss: 0.7770 - accuracy: 0.7859\n",
      "Epoch 4/15\n",
      "393/393 [==============================] - 115s 293ms/step - loss: 0.4701 - accuracy: 0.8698\n",
      "Epoch 5/15\n",
      "393/393 [==============================] - 116s 294ms/step - loss: 0.2650 - accuracy: 0.9294\n",
      "Epoch 6/15\n",
      "393/393 [==============================] - 115s 294ms/step - loss: 0.1425 - accuracy: 0.9634\n",
      "Epoch 7/15\n",
      "393/393 [==============================] - 115s 292ms/step - loss: 0.0823 - accuracy: 0.9798\n",
      "Epoch 8/15\n",
      "393/393 [==============================] - 116s 294ms/step - loss: 0.0516 - accuracy: 0.9877\n",
      "Epoch 9/15\n",
      "393/393 [==============================] - 115s 293ms/step - loss: 0.0357 - accuracy: 0.9914\n",
      "Epoch 10/15\n",
      "393/393 [==============================] - 116s 294ms/step - loss: 0.0275 - accuracy: 0.9934\n",
      "Epoch 11/15\n",
      "393/393 [==============================] - 115s 293ms/step - loss: 0.0218 - accuracy: 0.9949\n",
      "Epoch 12/15\n",
      "393/393 [==============================] - 116s 296ms/step - loss: 0.0201 - accuracy: 0.9950\n",
      "Epoch 13/15\n",
      "393/393 [==============================] - 116s 295ms/step - loss: 0.0216 - accuracy: 0.9943\n",
      "Epoch 14/15\n",
      "393/393 [==============================] - 116s 294ms/step - loss: 0.0233 - accuracy: 0.9935\n",
      "Epoch 15/15\n",
      "393/393 [==============================] - 116s 294ms/step - loss: 0.0239 - accuracy: 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5718fde10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_tf_data.shuffle(10000).batch(512),\n",
    "    epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify text\n",
    "\n",
    "Now we classify text with \"```Paris```\", \"```Hilton Hotel```\", and \"```Paris Hilton```\".<br>\n",
    "Only \"```Paris Hilton```\" will be categorized as ```ENTERTAINMENT```, because 2-gram word \"```Paris Hilton```\" frequently occurs in ```ENTERTAINMENT``` article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TRAVEL']\n",
      "['TRAVEL']\n",
      "['ENTERTAINMENT']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_text(headline, description):\n",
    "    test_X = tf.convert_to_tensor(\n",
    "    [\n",
    "        [headline],\n",
    "        [description]\n",
    "    ])\n",
    "    test_y_one_hot = model(test_X)\n",
    "    test_y_idx = np.argmax(test_y_one_hot, axis=-1)\n",
    "    test_y = [y_df.columns[i] for i in test_y_idx]\n",
    "    return test_y\n",
    "\n",
    "print(classify_text(\n",
    "    \"report about paris\",\n",
    "    \"paris is brilliant\"\n",
    "))\n",
    "print(classify_text(\n",
    "    \"report about hilton hotel\",\n",
    "    \"hilton hotel is brilliant\"\n",
    "))\n",
    "print(classify_text(\n",
    "    \"report about paris hilton\",\n",
    "    \"paris hilton is brilliant\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example will classify text with \"```Michael Jackson```\", \"```Michael Avenatti```\", and \"```Ronny Jackson```\".<br>\n",
    "Each of text includes either of \"```Michael```\" or \"```Jackson```\", or both of these. But the results will differ, because these 2-gram phrases have different occurrences in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WELLNESS']\n",
      "['MEDIA']\n",
      "['ENTERTAINMENT']\n"
     ]
    }
   ],
   "source": [
    "print(classify_text(\n",
    "    \"report about michael jackson\",\n",
    "    \"michael jackson is wise and honest\"\n",
    "))\n",
    "print(classify_text(\n",
    "    \"report about michael avenatti\",\n",
    "    \"michael avenatti is wise and honest\"\n",
    "))\n",
    "print(classify_text(\n",
    "    \"report about ronny jackson\",\n",
    "    \"ronny jackson is wise and honest\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
