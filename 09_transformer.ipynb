{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Machine Translation Example)\n",
    "\n",
    "Transformer is SOTA (state-of-the-art) model and used in today's a lot of successful works in neural methods.<br>\n",
    "Finally I'll implement transformer using previously learned architectures - such as, language modeling, encoder-decoder, and attention.\n",
    "\n",
    "As you saw in [exercise 08](./08_attention.ipynb), attention captures the distant relationship and contexts in sequences.<br>\n",
    "Transformer is motivated by this successful architecture.\n",
    "\n",
    "![Attention](./images/attend_image.png)<br>\n",
    "*From : \"08 Attention (Machine Translation Example)\"*\n",
    "\n",
    "In the famous paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\", attention is also used even for getting contexts in encoder and decoder, insetad of using RNN architecture. (As you saw in [exercise 08](./08_attention.ipynb), we have used RNN architecture (GRU gate) for getting contexts in encoder and decoder. In below architecture, there's no RNN layers.)<br>\n",
    "Total 3 attend layers (encoder's self-attention, decoder's self-attention, and encoder-decoder attention) are then applied in this network.\n",
    "\n",
    "![Transformer](./images/transformer.png)<br>\n",
    "*From : \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani, et al., 2017)*\n",
    "\n",
    "Now let's see the details of this architecture.\n",
    "\n",
    "Unlike [exercise 08](./08_attention.ipynb), it applies the following attention (called \"**scaled dot-product attention**\") in these 3 parts of attend layers.<br>\n",
    "As you can see below, this model measures the similarity by the dot-product operation, and only 3 networks for composing query, key, and value will be trained. When the query vector and key vector are similar, it will have a large value of dot product between these vectors. Such like soft attention (in [exercise 08](./08_attention.ipynb)), the matrix $Q \\cdot K^T$ will then have the relationship mapping (weight's mapping) between input1's query and input2's key. Finally, by applying dot-product operation again between this weight's vector and value vector of each features, the final feature vectors will be obtained in each token.<br>\n",
    "To say intuitively, first dot-product operation asks each keys by queries in the sequence, and then composes the objectives by combining between its results and values by the second dot-product operation.\n",
    "\n",
    "![Multi-head attention](./images/multi_head_attention.png)\n",
    "\n",
    "In the attend layer in encoding and decoding (see below), input1 and input2 are the same sequence. This architecture is called **self-attention**.<br>\n",
    "For instance, the word \"this\" in \"this is a pen\" will have the close relationship with the word \"pen\". The self-attention captures this kind of self-regressive relations in the sequence.<br>\n",
    "By capturing these relations, the sequence will be well annotated in encoder and decoder, instead of using RNN layer.\n",
    "\n",
    "![Self-attention parts](./images/transformer_self_attention.png)\n",
    "\n",
    "> Note : In the attention layer in decoder, each time segment should not refer to the future segment. It then applies the masked attention, instead of applying fully-connected attention. (See below code for details.)\n",
    "\n",
    "In this tutorial, we'll see the implementation of each part in this transofrmer architecture, and train the model for machine translation task.<br>\n",
    "For your learning purpose, I'll implement attention layer from scratch.\n",
    "\n",
    "By the design of this architecture, transformers will have the ability to capture the distant contexts and we can also expect to have the ability to capture more difficult contexts rather than RNN-based encoder-decoder. As I have mentioned above, transformer is today's key part in SOTA (state-of-the-art) language models and a lot of today's famous algorithms (such as, BERT, T5, GPT, etc) use transformers in its model architectures.<br>\n",
    "As you saw in [basic language model's example](./05_language_model_basic.ipynb), you can also apply unsupervised approach - such as, next word's prediction, masked word's prediction - in this transformer architecture. By this unsupervised way, a lot of today's algorithms learn a lot of language properties with large corpus, and these can also then be fine-tuned for specific downstream tasks with only small amount of labeled data by transfer approach.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/nlp-tutorials/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.6.2 numpy nltk matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I use Engligh-French dataset by [Anki](https://www.manythings.org/anki/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip fra-eng.zip -d fra-eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\r\n",
      "Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\r\n",
      "Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\r\n",
      "Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\r\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 fra-eng/fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197463 fra-eng/fra.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fra-eng/fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Va !', 'Go.'], dtype='<U349')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "pathobj = Path(\"fra-eng/fra.txt\")\n",
    "text_all = pathobj.read_text(encoding=\"utf-8\")\n",
    "lines = text_all.splitlines()\n",
    "train_data = [line.split(\"\\t\") for line in lines]\n",
    "train_data = np.array(train_data)[:,[1,0]]\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training set, text length in the latter part is longer (and includes multiple sentences) than the former part.<br>\n",
    "Therefore I shuffle entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Il y a quelques années, je me suis inscrit dans une salle de sport pour perdre du poids.',\n",
       "       'I joined a gym a few years back to lose weight.'], dtype='<U349')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(train_data)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data consists of multiple sentences, it converts to a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tsmatsuz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "tokenizer_en = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "tokenizer_fr = nltk.data.load(\"tokenizers/punkt/french.pickle\")\n",
    "fr_list = []\n",
    "en_list = []\n",
    "for x in train_data:\n",
    "    x1 = tokenizer_fr.tokenize(x[0])\n",
    "    x2 = tokenizer_en.tokenize(x[1])\n",
    "    if len(x1) == len(x2):\n",
    "        fr_list += x1\n",
    "        en_list += x2\n",
    "train_data = np.column_stack((fr_list, en_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), I standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation except \" ' \" (e.g, Ken's bag, ces't, ...)\n",
    "\n",
    "> Note : To make simplify, I have skipped other pre-processing, such as, N-gram detection (see [this example](./04_ngram_cnn.ipynb)), polysemy processing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['il y a quelques années je me suis inscrit dans une salle de sport pour perdre du poids',\n",
       "       'i joined a gym a few years back to lose weight'], dtype='<U250')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "train_data = np.char.lower(train_data)\n",
    "train_data = np.char.replace(train_data, \"-\", \" \")\n",
    "for x in string.punctuation.replace(\"'\", \"\"):\n",
    "    train_data = np.char.replace(train_data, x, \"\")\n",
    "for x in \"«»\":\n",
    "    train_data = np.char.replace(train_data, x, \"\")\n",
    "train_data = np.char.strip(train_data)\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ```[START]``` and ```[END]``` tokens in string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[START] il y a quelques années je me suis inscrit dans une salle de sport pour perdre du poids [END]',\n",
       "       '[START] i joined a gym a few years back to lose weight [END]'],\n",
       "      dtype='<U264')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.array([[\" \".join([\"[START]\", x, \"[END]\"]), \" \".join([\"[START]\", y, \"[END]\"])] for x, y in train_data])\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequence inputs\n",
    "\n",
    "We will generate the sequence of word's indices (i.e, tokenize) from text.\n",
    "\n",
    "![Index vectorize](images/index_vectorize2.png?raw=true)\n",
    "\n",
    "First, we build word's vectorizer for both source text (French) and target text (English) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_word = 10000\n",
    "source_seq_len = 45\n",
    "target_seq_len = 38\n",
    "\n",
    "source_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_word,\n",
    "    output_sequence_length=source_seq_len, # maximum length of sequences\n",
    "    output_mode=\"int\")\n",
    "source_vectorization.adapt(train_data[:,0])\n",
    "\n",
    "target_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_word,\n",
    "    output_sequence_length=target_seq_len, # maximum length of sequences\n",
    "    output_mode=\"int\")\n",
    "target_vectorization.adapt(train_data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we vectorize sentence into integer's sequence as follows.<br>\n",
    "In this example, we create fixed size of sequence (padded by 0) in each row.\n",
    "\n",
    "![Index vectorize](images/index_vectorize2.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   3   14   51   18  202  795    4   27   25 6076   32   23  899    5\n",
      " 1305   26  546   42  832    2    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0], shape=(45,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[   2    4 2584    8 2895    8  289  259  117    6  524  768    3    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0], shape=(38,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "train_source = source_vectorization(train_data[:,0])\n",
    "train_target = target_vectorization(train_data[:,1])\n",
    "# print first row\n",
    "print(train_source[0])\n",
    "print(train_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_data = tf.data.Dataset.from_tensor_slices((train_source, train_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate label (y) for training.\n",
    "\n",
    "We should finally predict the next word in target sentence (English).<br>\n",
    "We then create the following label (y) in each row.\n",
    "\n",
    "<u>input - list of current words</u> :\n",
    "```\n",
    "[2, 7, 5, 3, 0, ... , 0]\n",
    "```\n",
    "\n",
    "<u>output (y) - list of next words</u> :\n",
    "```\n",
    "[7, 5, 3, 0, 0, ... , 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** X *****\n",
      "(<tf.Tensor: shape=(45,), dtype=int64, numpy=\n",
      "array([   3,   14,   51,   18,  202,  795,    4,   27,   25, 6076,   32,\n",
      "         23,  899,    5, 1305,   26,  546,   42,  832,    2,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(38,), dtype=int64, numpy=\n",
      "array([   2,    4, 2584,    8, 2895,    8,  289,  259,  117,    6,  524,\n",
      "        768,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0])>)\n",
      "***** y *****\n",
      "tf.Tensor(\n",
      "[   4 2584    8 2895    8  289  259  117    6  524  768    3    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0], shape=(38,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def add_label(x_source, x_target):\n",
    "    y = tf.slice(x_target, begin=[1], size=[len(x_target) - 1])\n",
    "    y = tf.concat([y, [0]], axis=0)\n",
    "    return (x_source, x_target), y\n",
    "train_tf_data = train_tf_data.map(lambda src, tar: add_label(src, tar))\n",
    "# print first row\n",
    "for x, y in train_tf_data.take(1):\n",
    "    print(\"***** X *****\")\n",
    "    print(x)\n",
    "    print(\"***** y *****\")\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert label y into one-hot vector (shape (max_word,)).\n",
    "\n",
    "y (before) :\n",
    "```\n",
    "[4, 1, 3, 0]\n",
    "```\n",
    "\n",
    "y (after) :\n",
    "```\n",
    "[\n",
    "  [0,0,0,0,1, ... ,0],\n",
    "  [0,1,0,0,0, ... ,0],\n",
    "  [0,0,0,1,0, ... ,0],\n",
    "  [1,0,0,0,0, ... ,0]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]], shape=(38, 10000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(x, y):\n",
    "    y_new = tf.one_hot(y, depth=max_word)\n",
    "    return x, y_new\n",
    "train_tf_data = train_tf_data.map(lambda x, y: to_one_hot(x, y))\n",
    "# print y in first row\n",
    "for x, y in train_tf_data.take(1):\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In the formal algorithms of transformer, the set of embedded tokens are encoded by positions.\n",
    "\n",
    "![Positional encoding](./images/transformer_positional_encoding.png)\n",
    "\n",
    "If there's no positional encoding, the sequence will be treated as a bag of tokens in neural networks. The positional information is needed for position-aware processing in attention.\n",
    "\n",
    "There exist several ways (variations) for positional encoding.<br>\n",
    "In this example, I'll apply the following positional encoding method (called **sinusoidal positional encoding**), which is introduced in the [original paper](https://arxiv.org/abs/1706.03762) of transformer.\n",
    "\n",
    "1. The positional vector $PE(t)$ is :<br>\n",
    "$ PE(t,2i) = \\sin(t / 10000^{2i / d_e}) $<br>\n",
    "$ PE(t,2i+1) = \\cos(t / 10000^{2i / d_e}) $<br>\n",
    "for $0 \\leq i \\lt d_e / 2$<br>\n",
    "where $t = \\{0, 1, \\ldots\\}$ is position and $d_e$ is embedding dimemsion.\n",
    "2. For $t$-th token of the sequence, the embedding $E(t) \\in \\mathbb{R}^{d_e}$ then becomes $ E(t) + PE(t) $.\n",
    "\n",
    "> Note : Here we assume that $d_e$ (embedding dimension) is an even number. (When it's an odd number, the dimension between $E(t)$ and $PE(t)$ differs.)\n",
    "\n",
    "By applying this positional encoding, we can expect that the attention network will easily learn the position in each tokens, since there always exist a $ 2 \\times 2 $ matrix $\\mathbf{M}_{ik}$ (depending on $i$ and $k$) which satisfies\n",
    "\n",
    "$$ \\begin{pmatrix} PE(t+k,2i)\\\\PE(t+k,2i+1) \\end{pmatrix} = \\mathbf{M}_{ik} \\begin{pmatrix} PE(t,2i)\\\\PE(t,2i+1) \\end{pmatrix} $$\n",
    "\n",
    "for any $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, seq_len, embedding_dim, **kwargs):\n",
    "        assert(embedding_dim % 2 == 0)\n",
    "\n",
    "        super().__init__()\n",
    "        # 1 / 10000^{2i / d_e} - Shape is (embedding_dim / 2, )\n",
    "        interval = 1.0 / (10000**(tf.range(0, embedding_dim, 2.0) / embedding_dim))\n",
    "        # t - Shape is (seq_len, )\n",
    "        position = tf.cast(tf.range(0, seq_len), tf.float32)\n",
    "        # t / 10000^{2i / d_e} - Shape is (seq_len, embedding_dim / 2)\n",
    "        radian = position[:, None] * interval[None, :]\n",
    "        # sin(t / 10000^{2i / d_e}) - Shape is (seq_len, embedding_dim / 2, 1)\n",
    "        sin = tf.expand_dims(tf.sin(radian), -1)\n",
    "        # cos(t / 10000^{2i / d_e}) - Shape is (seq_len, embedding_dim / 2, 1)\n",
    "        cos = tf.expand_dims(tf.cos(radian), -1)\n",
    "        # PE - Shape is (seq_len, embedding_dim / 2, 2)\n",
    "        pe_tmp =  tf.concat((sin, cos), axis=-1)\n",
    "        # Reshape PE - Shape is (seq_len, embedding_dim)\n",
    "        s = pe_tmp.shape[0]\n",
    "        d = pe_tmp.shape[1]\n",
    "        self.pe = tf.reshape(pe_tmp, [s, d * 2])\n",
    "\n",
    "    # This method is needed to support masking in outputs.\n",
    "    # see https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs._keras_mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the output of positional encoding layer.<br>\n",
    "In the following example, positional vectors will become :\n",
    "\n",
    "```\n",
    "[\n",
    "  [sin(0), cos(0), sin(0/100), cos(0/100)],\n",
    "  [sin(1), cos(1), sin(1/100), cos(1/100)],\n",
    "  [sin(2), cos(2), sin(2/100), cos(2/100)],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### positional vector #####\n",
      "tf.Tensor(\n",
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.841471    0.5403023   0.00999983  0.99995   ]\n",
      " [ 0.90929747 -0.4161468   0.01999867  0.9998    ]], shape=(3, 4), dtype=float32)\n",
      "##### output #####\n",
      "tf.Tensor(\n",
      "[[[ 1.         3.         3.         5.       ]\n",
      "  [ 5.841471   6.5403023  7.0099998  8.99995  ]\n",
      "  [ 9.909298   9.583853  11.019999  12.9998   ]]\n",
      "\n",
      " [[13.        15.        15.        17.       ]\n",
      "  [17.84147   18.540302  19.01      20.99995  ]\n",
      "  [21.909298  21.583853  23.019999  24.9998   ]]], shape=(2, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test = PositionalEncoding(3, 4)\n",
    "print(\"##### positional vector #####\")\n",
    "print(test.pe)\n",
    "# The input shape is [batch_size, seq_len, embedding_dim]\n",
    "x = tf.constant([\n",
    "    [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0]],\n",
    "    [[13.0, 14.0, 15.0, 16.0], [17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0]]\n",
    "], dtype=tf.float32)\n",
    "mask = tf.keras.layers.Masking(mask_value=0., input_shape=(3, 4))\n",
    "x = mask(x)\n",
    "y = test(x)\n",
    "print(\"##### output #####\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multi-head and Scaled Dot-Product Attention\n",
    "\n",
    "Next I'll implement attention layer as follows.\n",
    "\n",
    "For the semantics of this model, see above description.\n",
    "\n",
    "To make multiple attention work in parallel, model dimension (here 256) is divided into multiple heads (here 8), and each head will then have ```model_dim / head_num``` dimension (here 32). (Finally, these separated heads are concatenated and then applied dense network to obtain model dimension's result.)<br>\n",
    "This technique will make our model have rich expression without losing the computing costs.\n",
    "\n",
    "![Multi-head attention](./images/transformer_attention.png)\n",
    "\n",
    "> Note : For the purpose of your learning, I'll implement the scaled dot-product attention layer from scratch, but you can use built-in ```tf.keras.layers.MultiHeadAttention()``` for multi-head attention implementation in TensorFlow.\n",
    "\n",
    "> Note : I'll describe the purpose of ```use_causal_mask``` property later (in decoder implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Initializes a MultiHeadAttention keras Layer object.\n",
    "\n",
    "    Args:\n",
    "        attention_dim (int): The dimension within attention unit.\n",
    "        use_causal_mask (bool): Whether to mask the future tokens.\n",
    "            (This will be enabled in decoder side.)\n",
    "        **kwargs:\n",
    "    \"\"\"\n",
    "    def __init__(self, attention_dim, num_heads, use_causal_mask=False, **kwargs):\n",
    "        assert(attention_dim % num_heads == 0)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.head_dim = int(attention_dim / num_heads)\n",
    "        self.use_causal_mask = use_causal_mask\n",
    "        self.q_layer = tf.keras.layers.Dense(\n",
    "            attention_dim, name=\"q\")\n",
    "        self.k_layer = tf.keras.layers.Dense(\n",
    "            attention_dim, name=\"k\")\n",
    "        self.v_layer = tf.keras.layers.Dense(\n",
    "            attention_dim, name=\"v\")\n",
    "        self.softmax = tf.keras.layers.Softmax(\n",
    "            axis=2\n",
    "        )\n",
    "        self.output_linear = tf.keras.layers.Dense(\n",
    "            attention_dim, use_bias=False, name=\"o\")\n",
    "        super().__init__()\n",
    "\n",
    "    # This method is needed to support masking in outputs.\n",
    "    # see https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs._keras_mask\n",
    "\n",
    "    def call(self, inputs, input2=None, training=False):\n",
    "        input1 = inputs\n",
    "        if input2 is None:\n",
    "            input2 = input1\n",
    "\n",
    "        batch_size = tf.shape(input1)[0]\n",
    "        seq_len1 = tf.shape(input1)[1]\n",
    "        seq_len2 = tf.shape(input2)[1]\n",
    "\n",
    "        # Apply query/key/value net\n",
    "        q = self.q_layer(input1, training=training)\n",
    "        k = self.k_layer(input2, training=training)\n",
    "        v = self.v_layer(input2, training=training)\n",
    "\n",
    "        # Divide shape [batch_size, seq_len, attention_dim] into [batch_size, seq_len, num_heads, attention_dim / num_heads]\n",
    "        q = tf.reshape(q, [-1, seq_len1, self.num_heads, self.head_dim])\n",
    "        k = tf.reshape(k, [-1, seq_len2, self.num_heads, self.head_dim])\n",
    "        v = tf.reshape(v, [-1, seq_len2, self.num_heads, self.head_dim])\n",
    "\n",
    "        # Q K^T\n",
    "        score = tf.einsum(\"bihd,bjhd->bijh\", q, k)\n",
    "        \n",
    "        # Q K^T * 1/sqrt(d)\n",
    "        score = score / self.head_dim**0.5\n",
    "\n",
    "        # Generate mask for applying\n",
    "        # (mask shape becomes [seq_len1, seq_len2])\n",
    "        if self.use_causal_mask:\n",
    "            # Note : when use_causal_mask=True, the shape of input1 should be same as input2\n",
    "            assert(seq_len1 == seq_len2)\n",
    "            mask = tf.sequence_mask(\n",
    "                lengths=tf.range(1, seq_len1 + 1),\n",
    "                maxlen=seq_len1,\n",
    "                dtype=tf.float32)\n",
    "        else:\n",
    "            mask = tf.fill([seq_len1, seq_len2], 1.0)\n",
    "        # (mask shape becomes [batch_size, seq_len1, seq_len2, num_heads])\n",
    "        mask = tf.expand_dims(mask, axis=0)\n",
    "        mask = tf.expand_dims(mask, axis=3)\n",
    "        mask = tf.tile(mask, [batch_size, 1, 1, self.num_heads])\n",
    "        # (add masking for input2 sequence)\n",
    "        input2_mask = input2._keras_mask # shape [batch_size, seq_len2]\n",
    "        input2_mask = tf.cast(input2_mask, tf.float32)\n",
    "        input2_mask = tf.expand_dims(input2_mask, axis=1) # shape [batch_size, 1, seq_len2]\n",
    "        input2_mask = tf.expand_dims(input2_mask, axis=3) # shape [batch_size, 1, seq_len2, 1]\n",
    "        input2_mask = tf.tile(input2_mask, [1, seq_len1, 1, self.num_heads]) # shape [batch_size, seq_len1, seq_len2, num_heads]\n",
    "        mask = tf.math.multiply(mask, input2_mask)\n",
    "\n",
    "        # Apply softmax (on axis 2) : softmax(Q K^T * 1/sqrt(d))\n",
    "        wmat = self.softmax(\n",
    "            inputs=score,\n",
    "            mask=mask,\n",
    "        )\n",
    "\n",
    "        # Softmax(Q K^T * 1/sqrt(d)) V\n",
    "        out = tf.einsum(\"bijh,bjhd->bihd\", wmat, v)\n",
    "\n",
    "        # Concatenate all heads and apply linear\n",
    "        out = tf.reshape(out, [batch_size, seq_len1, self.attention_dim])\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Now let's implement encoder side with previously generated layers.<br>\n",
    "Same as [exercise 08](./08_attention.ipynb), the purpose of encoder is to generate the contexts of source sequence (French text). However, unlike exercise 08, we don't use RNN (GRU) and apply the scaled dot-product attention (self-attention) instead.\n",
    "\n",
    "![Transformer](./images/transformer.png)\n",
    "\n",
    "As you can see above, the encoder in transformer (left side in above picture) has the repeated layers. (In above picture, it shows with \"Nx\".)<br>\n",
    "For this reason, we'll first implement the following repeated layer component.\n",
    "\n",
    "![Encoding Layer](./images/transformer_encoding_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleEncodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, seq_len, model_dim, num_heads, hidden_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.norm = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        self.output_dense1 = tf.keras.layers.Dense(\n",
    "            hidden_dim,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.output_dense2 = tf.keras.layers.Dense(\n",
    "            model_dim,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "    # This method is needed to support masking in outputs.\n",
    "    # see https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs._keras_mask\n",
    "\n",
    "    # This method keeps input's masking ...\n",
    "    def call(self, inputs, training=False):\n",
    "        # Attention\n",
    "        attention_outputs = self.self_attention(inputs, training=training)\n",
    "        # Add & Layer norm (with dropout)\n",
    "        attention_outputs = self.dropout(attention_outputs)\n",
    "        attention_outputs = self.add([attention_outputs, inputs])\n",
    "        attention_outputs = self.norm(attention_outputs)\n",
    "        # Feed Forward\n",
    "        outputs = self.output_dense1(attention_outputs, training=training)\n",
    "        outputs = self.output_dense2(outputs, training=training)\n",
    "        # Add & Layer norm (with dropout)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.add([outputs, attention_outputs])\n",
    "        outputs = self.norm(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With previously generated layer, now we implement the encoder as follows.\n",
    "\n",
    "![Encoding Layer](./images/transformer_encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, seq_len, model_dim, num_layers, num_heads, hidden_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            model_dim,\n",
    "            mask_zero=True,\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=model_dim,\n",
    "        )\n",
    "        self.encoding_layers = [\n",
    "            SingleEncodingLayer(\n",
    "                seq_len=seq_len,\n",
    "                model_dim=model_dim,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "            )\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "    # This method keeps input's masking ...\n",
    "    def call(self, source_inputs, training=False):\n",
    "        # Embedding\n",
    "        outputs = self.embedding(source_inputs, training=training)\n",
    "        # Positional encoding\n",
    "        outputs = self.pos_encoding(outputs, training=training)\n",
    "        # Apply SingleEncodingLayer (multiple times)\n",
    "        for i in range(self.num_layers):\n",
    "            outputs = self.encoding_layers[i](outputs, training=training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Next implement decoder side.<br>\n",
    "Same as encoder, we'll first implement the repeated layer component as the following picture shows.\n",
    "\n",
    "![Decoding Layer](./images/transformer_decoding_layer.png)\n",
    "\n",
    "In the self-attention layer in decoder, i-th token should be attended by tokens 1, 2, ... , i - 1 (because the decoder doesn't know future words), and it should then set the property ```use_causal_mask=True```.<br>\n",
    "Same as exercise 08, the attended output between encoded result and decoded result will then be generated with the next encoder-decoder attention by feeding both encoder's outputs and decoder's outputs. (But it uses the scaled dot-product attention unlike exercise 08.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDecodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, seq_len, model_dim, num_heads, hidden_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "            use_causal_mask=True,\n",
    "        )\n",
    "        self.cross_attention = MultiHeadAttention(\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.norm = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        self.feed_dense1 = tf.keras.layers.Dense(\n",
    "            hidden_dim,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.feed_dense2 = tf.keras.layers.Dense(\n",
    "            model_dim,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "    # This method is needed to support masking in outputs.\n",
    "    # see https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs._keras_mask\n",
    "\n",
    "    # This method keeps target input's masking ...\n",
    "    def call(self, inputs, enc_outputs, training=False):\n",
    "        # Self-attention\n",
    "        attention_outputs = self.self_attention(inputs, training=training)\n",
    "        # Add & Layer norm (with dropout)\n",
    "        attention_outputs = self.dropout(attention_outputs)\n",
    "        attention_outputs = self.add([attention_outputs, inputs])\n",
    "        attention_outputs = self.norm(attention_outputs)\n",
    "        # Encoder-decoder attention\n",
    "        scored_outputs = self.cross_attention(\n",
    "            attention_outputs,\n",
    "            input2=enc_outputs,\n",
    "            training=training,\n",
    "        )\n",
    "        # Add & Layer norm (with dropout)\n",
    "        scored_outputs = self.dropout(scored_outputs)\n",
    "        scored_outputs = self.add([scored_outputs, attention_outputs])\n",
    "        scored_outputs = self.norm(scored_outputs)\n",
    "        # Feed Forward\n",
    "        outputs = self.feed_dense1(scored_outputs, training=training)\n",
    "        outputs = self.feed_dense2(outputs, training=training)\n",
    "        # Add & Layer norm (with dropout)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.add([outputs, scored_outputs])\n",
    "        outputs = self.norm(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build decoder with previous layer component.\n",
    "\n",
    "![Decoder](./images/transformer_decoder.png)\n",
    "\n",
    "The outputs is used for predicting the next vocabulary by one-hot outputs, and the output's shape will then be ```(batch_size, sequence_length, vocabulary_size)```.\n",
    "\n",
    "> Note : In this example, the final softmax will be applied in loss computation, and here I don't implement this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, seq_len, model_dim, num_layers, num_heads, hidden_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            model_dim,\n",
    "            mask_zero=True,\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=model_dim,\n",
    "        )\n",
    "        self.decoding_layers = [\n",
    "            SingleDecodingLayer(\n",
    "                seq_len=seq_len,\n",
    "                model_dim=model_dim,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "            )\n",
    "            for _ in range(num_layers)]\n",
    "        self.output_dense = tf.keras.layers.Dense(\n",
    "            vocab_size,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "    # This method keeps target input's masking ...\n",
    "    def call(self, target_inputs, enc_outputs, training=False):\n",
    "        # Embedding\n",
    "        outputs = self.embedding(target_inputs, training=training)\n",
    "        # Positional encoding\n",
    "        outputs = self.pos_encoding(outputs, training=training)\n",
    "        # Apply SingleEncodingLayer (multiple times)\n",
    "        for i in range(self.num_layers):\n",
    "            outputs = self.decoding_layers[i](outputs, enc_outputs, training=training)\n",
    "        # Linear\n",
    "        # (batch_size, seq_len, model_dim) -> (batch_size, seq_len, vocab_size)\n",
    "        outputs = self.output_dense(outputs, training=training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (Transformer)\n",
    "\n",
    "Using previous models, now we build the training loop.\n",
    "\n",
    "First we set the following parameters in this training.<br>\n",
    "In this example, the training dataset consists of single sentences (i.e, not so long and difficult text) and I have reduced parameters to speed up the training, compared with the parameters which is used in the original paper.\n",
    "\n",
    "> Note : By increasing parameters, transformers will have the ability to capture more difficult contexts, but it'll need more training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "hidden_dim = 1024\n",
    "\n",
    "### In the original paper, the following parameters are used.\n",
    "#model_dim = 512\n",
    "#num_heads = 8\n",
    "#num_layers = 6\n",
    "#hidden_dim = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the following step-to-step learning rate is used for this model.\n",
    "\n",
    "$$ \\verb|lrate| = d^{-0.5}_{\\verb|model|} \\cdot \\min(\\verb|step_num|^{-0.5},\\verb|step_num|\\cdot\\verb|warmup_steps|^{-1.5}) $$\n",
    "\n",
    "So we will also use the same progressive learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule\n",
    "class TransformerLRScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, model_dim, warmup_steps=4000):\n",
    "        self.model_dim = float(model_dim)\n",
    "        self.warmup_steps = float(warmup_steps)\n",
    "    def __call__(self, step):\n",
    "        step = float(step)\n",
    "        if step == 0.0:\n",
    "            val1 = 0.0\n",
    "        else:\n",
    "            val1 = 1.0 / (step ** 0.5)\n",
    "        val2 = step / (self.warmup_steps ** 1.5)\n",
    "        return min(val1,val2) / (self.model_dim ** 0.5)\n",
    "\n",
    "learning_rate = TransformerLRScheduler(model_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, put it all together and run training as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - loss   2.953753\n",
      "Epoch 2/10 - loss   2.208414\n",
      "Epoch 3/10 - loss   1.918675\n",
      "Epoch 4/10 - loss   1.712586\n",
      "Epoch 5/10 - loss   1.688345\n",
      "Epoch 6/10 - loss   1.619679\n",
      "Epoch 7/10 - loss   1.599739\n",
      "Epoch 8/10 - loss   1.561381\n",
      "Epoch 9/10 - loss   1.461882\n",
      "Epoch 10/10 - loss   1.493265\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_epoch = 10\n",
    "shuffle_buffer_size = 5000\n",
    "train_batch_size = 32\n",
    "loss_records = []\n",
    "\n",
    "enc_model = Encoder(\n",
    "    vocab_size=max_word,\n",
    "    seq_len=source_seq_len,\n",
    "    model_dim=model_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    ")\n",
    "dec_model = Decoder(\n",
    "    vocab_size=max_word,\n",
    "    seq_len=target_seq_len,\n",
    "    model_dim=model_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    ")\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.98,\n",
    "    epsilon=1e-9)\n",
    "\n",
    "### I don't use train_tf_data.repeat(n_epoch) and run shuffle in each epoch,\n",
    "### because I want to check result in each epoch\n",
    "for epoch in range(n_epoch):\n",
    "    tf_data_epoch = train_tf_data.shuffle(shuffle_buffer_size).batch(train_batch_size)\n",
    "    iterator = iter(tf_data_epoch)\n",
    "    while True:\n",
    "        with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape:\n",
    "            # get next batch\n",
    "            try:\n",
    "                input_batch_x, input_batch_y = iterator.get_next()\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                loop_f = False\n",
    "                break\n",
    "            input_source, input_target = input_batch_x\n",
    "            # process encoder\n",
    "            enc_outputs = enc_model(\n",
    "                source_inputs=input_source,\n",
    "                training=True)\n",
    "            # process decoder\n",
    "            logits = dec_model(\n",
    "                target_inputs=input_target,\n",
    "                enc_outputs=enc_outputs,\n",
    "                training=True)\n",
    "            # calculate masked loss\n",
    "            loss = loss_func(\n",
    "                input_batch_y,\n",
    "                logits) # shape of loss is (batch_size, sequence_size)\n",
    "            mask_float = tf.cast(logits._keras_mask, tf.float32) # True -> 1.0, False -> 0.0\n",
    "            loss = loss * mask_float\n",
    "            loss = tf.math.reduce_sum(loss, axis=1) # shape of loss is (batch_size, )\n",
    "            loss = loss / tf.math.reduce_sum(mask_float, axis=1)\n",
    "            # get gradient\n",
    "            grad = enc_tape.gradient(\n",
    "                loss,\n",
    "                enc_model.trainable_variables+dec_model.trainable_variables\n",
    "            )\n",
    "        # apply gradient\n",
    "        opt.apply_gradients(zip(grad, enc_model.trainable_variables+dec_model.trainable_variables))\n",
    "        # record result\n",
    "        batch_average_loss = tf.math.reduce_mean(loss).numpy().tolist()\n",
    "        print(\"Epoch {}/{} - loss {loss:10.6f}\".format(epoch+1, n_epoch, loss=batch_average_loss), end=\"\\r\")\n",
    "        loss_records.append(batch_average_loss)\n",
    "    # output last 100 loss average in epoch\n",
    "    print(\"Epoch {}/{} - loss {loss:10.6f}\".format(epoch+1, n_epoch, loss=np.average(loss_records[-100:])), end=\"\\r\")\n",
    "    # add line break\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training has completed, show how loss is optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde645985f8>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#loss_list = loss_records[:10000]\n",
    "loss_list = loss_records\n",
    "\n",
    "loss_average = []\n",
    "for i in range(len(loss_list)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if i < 50:\n",
    "        avg_list = loss_list[:i+1]\n",
    "    else:\n",
    "        avg_list = loss_list[i-49:i+1]\n",
    "    loss_average.append(np.average(avg_list))\n",
    "plt.plot(loss_list)\n",
    "plt.plot(loss_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Text\n",
    "\n",
    "Now translate text (French) to English with trained model.<br>\n",
    "Here I simply translate several sentences. (All these sentences are not in training set.)\n",
    "\n",
    "The metrics to evaluate text generation task is not so easy. (Because simply checking an exact match to a reference text is not optimal.)<br>\n",
    "Use some common metrics available in these cases, such as, BLEU or ROUGE.\n",
    "\n",
    "> Note : Here I use greedy search and this will sometimes lead to wrong sequence. For drawbacks and solutinos, see note in [this example](./05_language_model_basic.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def translate(sentence):\n",
    "    # preprocess french\n",
    "    input_text_fr = sentence\n",
    "    input_text_fr = input_text_fr.lower()\n",
    "    input_text_fr = \" \".join([\"[START]\", input_text_fr, \"[END]\"])\n",
    "\n",
    "    text_seq_fr = source_vectorization(input_text_fr)\n",
    "    text_seq_fr = tf.expand_dims(text_seq_fr, axis=0)\n",
    "\n",
    "    # process encoder\n",
    "    enc_outputs = enc_model(text_seq_fr)\n",
    "\n",
    "    # process decoder\n",
    "    vocab_list_en = target_vectorization.get_vocabulary()\n",
    "    token_idx = 0\n",
    "    text_list_en = [\"[START]\"]\n",
    "    end_num_en = target_vectorization(\"[END]\").numpy().tolist()[0]\n",
    "    while True:\n",
    "        text_seq_en = target_vectorization(\" \".join(text_list_en))\n",
    "        text_seq_en = tf.expand_dims(text_seq_en, axis=0)\n",
    "        y = dec_model(\n",
    "            target_inputs=text_seq_en,\n",
    "            enc_outputs=enc_outputs,\n",
    "        )\n",
    "        prev_idx_en = np.argmax(y[0][token_idx])\n",
    "        if prev_idx_en.item() == end_num_en:\n",
    "            break\n",
    "        next_word_en = vocab_list_en[prev_idx_en.item()]\n",
    "        print(next_word_en, end=\" \")\n",
    "        text_list_en.append(next_word_en)\n",
    "        token_idx += 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like guitar \n",
      "\n",
      "he lives in japan \n",
      "\n",
      "that pen is used to him \n",
      "\n",
      "this is my favorite song \n",
      "\n",
      "he drives a car and a new york \n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\"j'aime la guitare\") # i like guitar\n",
    "translate(\"il vit au japon\") # he lives in Japan\n",
    "translate(\"ce stylo est utilisé par lui\") # this pen is used by him\n",
    "translate(\"c'est ma chanson préférée\") # that's my favorite song\n",
    "translate(\"il conduit une voiture et va à new york\") # he drives a car and goes to new york"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
