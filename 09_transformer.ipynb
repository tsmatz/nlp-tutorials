{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Machine Translation Example)\n",
    "\n",
    "Transformer is popular SOTA (state-of-the-art) architecture and used in today's a lot of successful works in neural methods.<br>\n",
    "Finally we'll implement transformer using previously learned architectures - such as, language modeling, encoder-decoder, and attention.\n",
    "\n",
    "As you saw in [exercise 08](./08_attention.ipynb), attention captures the distant relationship and contexts in sequences.<br>\n",
    "Transformer is motivated by this successful architecture.\n",
    "\n",
    "![Attention](./images/attend_image.png)<br>\n",
    "*From : \"08 Attention (Machine Translation Example)\"*\n",
    "\n",
    "In this example, we will implement the architecture written in the famous paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\".<br>\n",
    "As you saw in [exercise 08](./08_attention.ipynb), we have used RNN architecture (GRU gate) for getting contexts in encoder and decoder. However, in this transformer architecture, attention is also used even for getting contexts in encoder and decoder, instead of using RNN architecture. (In below architecture, you will find that there's no RNN layers.)<br>\n",
    "Total 3 attend layers (encoder's self-attention, decoder's self-attention, and encoder-decoder cross attention) are then applied in this network.\n",
    "\n",
    "![Transformer](./images/transformer.png)<br>\n",
    "*From : \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani, et al., 2017)*\n",
    "\n",
    "Unlike soft attention in [exercise 08](./08_attention.ipynb), it applies the following attention - which is called \"**scaled dot-product attention**\" - in these 3 parts of attend layers.<br>\n",
    "As you can see below, this model measures the similarity by the dot-product operation, and 3 networks for composing query, key, and value will be trained. When the query vector and key vector are similar, it will have a large value of dot product between these vectors. Such like soft attention (in [exercise 08](./08_attention.ipynb)), the matrix $Q \\cdot K^T$ will then have the relationship mapping (weight's mapping) between query and key. Finally, by applying dot-product operation again between this result and value's vector, the final feature vectors will be obtained in each token.<br>\n",
    "To say intuitively, first dot-product operation asks each keys by queries in the sequence, and then composes the objectives by combining between its results and values by the second dot-product operation.\n",
    "\n",
    "![Multi-head attention](./images/multi_head_attention.png)\n",
    "\n",
    "In the attend layer in encoding and decoding (see below), input1 and input2 are the same sequence in above scaled dot-product attention. This architecture is called **self-attention**.<br>\n",
    "For instance, the word \"this\" in \"this is a pen\" will have the close relationship with the word \"pen\". The self-attention captures this kind of self-regressive relations in the sequence.<br>\n",
    "By capturing these relations, the sequence will be well annotated in both encoder and decoder, instead of using RNN layer.\n",
    "\n",
    "![Self-attention parts](./images/transformer_self_attention.png)\n",
    "\n",
    "> Note : On contrary, the above cross-attention layer is for machine translation task.\n",
    "\n",
    "> Note : In the self-attention layer in decoder, each time segment should not refer to the future segment. It then applies the masked attention, instead of applying fully-connected attention.<br>\n",
    "> Later I'll explain about this causal attention.\n",
    "\n",
    "By the design of this architecture, transformers will have the ability to capture the distant contexts and we can also expect to have the ability to capture more difficult contexts rather than RNN-based encoder-decoder.\n",
    "\n",
    "As I have mentioned above, transformer is today's key part for SOTA (state-of-the-art) language models and a lot of today's famous algorithms (such as, BERT, T5, GPT, etc) use transformers in its architectures.<br>\n",
    "As you saw in [basic language model's example](./05_language_model_basic.ipynb), we can train transformers with unlabeled data (i.e, self-supervised learning) - such as, next word's prediction, masked word's prediction. By this unsupervised fashion, a lot of today's algorithms learn a lot of language properties with existing large corpus (such as, Wikipedia), and can then be fine-tuned for downstream tasks with small number of labeled data - such as, sentiment detection, text classification, summarization, or modern instruction fine-tuning, etc. (See [here](https://tsmatz.wordpress.com/2022/10/24/huggingface-japanese-ner-named-entity-recognition/) for fine-tuning example in Hugging Face.)\n",
    "\n",
    "> Note : Transformers are categorized into 3 types : encoder-only, decoder-only, and encoder-decoder. (See [here](https://arxiv.org/abs/2304.13712) for the summary of encoder-only, decoder-only, and encoder-decoder language models.)<br>\n",
    "> For instance, you can use only encoder's architecture and apply classification for named entity recognitions. You may also use text encoding for image generation.<br>\n",
    "> A lot of today's popular large language models (LLMs) - such as, ChatGPT, LLaMA, etc - are decoder-only models. These are in-context generative models, in which the output is generated by using only input (prompt), and the context of input's text (prompting) is then very important to get the optimal results. These models are trained by 3-stages for practical use - pretraining (self-supervised learning), supervised fine-tuning (SFT), and alignment. (See [here](https://arxiv.org/abs/2203.02155) for these training strategies.)\n",
    "\n",
    "Now let's see the implementation of each part in this transofrmer architecture, and train the model for machine translation task.<br>\n",
    "For your learning purpose, I'll implement all layers (modules) from scratch, but you can also use built-in ```torch.nn.MultiheadAttention``` in PyTorch.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/nlp-tutorials/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1 torchtext==0.14.1 --extra-index-url https://download.pytorch.org/whl/cu114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I use Engligh-French dataset by [Anki](https://www.manythings.org/anki/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-19 07:59:20--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6720195 (6.4M) [application/zip]\n",
      "Saving to: ‘fra-eng.zip’\n",
      "\n",
      "fra-eng.zip         100%[===================>]   6.41M  11.6MB/s    in 0.6s    \n",
      "\n",
      "2023-02-19 07:59:21 (11.6 MB/s) - ‘fra-eng.zip’ saved [6720195/6720195]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fra-eng.zip\n",
      "  inflating: fra-eng/_about.txt      \n",
      "  inflating: fra-eng/fra.txt         \n"
     ]
    }
   ],
   "source": [
    "!unzip fra-eng.zip -d fra-eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\r\n",
      "Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\r\n",
      "Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\r\n",
      "Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\r\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 fra-eng/fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197463 fra-eng/fra.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fra-eng/fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Va !', 'Go.'], dtype='<U349')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "pathobj = Path(\"fra-eng/fra.txt\")\n",
    "text_all = pathobj.read_text(encoding=\"utf-8\")\n",
    "lines = text_all.splitlines()\n",
    "train_data = [line.split(\"\\t\") for line in lines]\n",
    "train_data = np.array(train_data)[:,[1,0]]\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training set, text length in the latter part is longer (and includes multiple sentences) than the former part.<br>\n",
    "Therefore I shuffle entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Je suis heureux que vous veniez.', \"I'm glad you're coming.\"],\n",
       "      dtype='<U349')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(train_data)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data consists of multiple sentences, it converts to a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tsmatsuz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "tokenizer_en = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "tokenizer_fr = nltk.data.load(\"tokenizers/punkt/french.pickle\")\n",
    "fr_list = []\n",
    "en_list = []\n",
    "for x in train_data:\n",
    "    x1 = tokenizer_fr.tokenize(x[0])\n",
    "    x2 = tokenizer_en.tokenize(x[1])\n",
    "    if len(x1) == len(x2):\n",
    "        fr_list += x1\n",
    "        en_list += x2\n",
    "train_data = np.column_stack((fr_list, en_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), I standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation except \" ' \" (e.g, Ken's bag, ces't, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['je suis heureux que vous veniez', \"i'm glad you're coming\"],\n",
       "      dtype='<U250')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "train_data = np.char.lower(train_data)\n",
    "train_data = np.char.replace(train_data, \"-\", \" \")\n",
    "for x in string.punctuation.replace(\"'\", \"\"):\n",
    "    train_data = np.char.replace(train_data, x, \"\")\n",
    "for x in \"«»\":\n",
    "    train_data = np.char.replace(train_data, x, \"\")\n",
    "train_data = np.char.strip(train_data)\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ```<start>``` and ```<end>``` tokens in string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<start> je suis heureux que vous veniez <end>',\n",
       "       \"<start> i'm glad you're coming <end>\"], dtype='<U264')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.array([[\" \".join([\"<start>\", x, \"<end>\"]), \" \".join([\"<start>\", y, \"<end>\"])] for x, y in train_data])\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequence inputs\n",
    "\n",
    "We will generate the sequence of word's indices (i.e, tokenize) from text.\n",
    "\n",
    "![Index vectorize](images/index_vectorize2.png)\n",
    "\n",
    "First we create a list of vocabulary (```vocab```) for both source text (French) and target text (English) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "max_word = 10000\n",
    "\n",
    "# create space-split tokenizer\n",
    "tokenizer = get_tokenizer(None)\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        tokens = tokenizer(text)\n",
    "        yield tokens\n",
    "\n",
    "# build vocabulary list for French\n",
    "vocab_fr = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data[:,0]),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "vocab_fr.set_default_index(vocab_fr[\"<unk>\"])\n",
    "\n",
    "# build vocabulary list for English\n",
    "vocab_en = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data[:,1]),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated token index is ```0, 1, ... , vocab_size - 1```.<br>\n",
    "Now I set ```vocab_size``` as a token id in padded positions for both French and English respctively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index_fr = vocab_fr.__len__()\n",
    "vocab_fr.append_token(\"<pad>\")\n",
    "\n",
    "pad_index_en = vocab_en.__len__()\n",
    "vocab_en.append_token(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list for both index-to-word and word-to-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_fr = vocab_fr.get_itos()\n",
    "stoi_fr = vocab_fr.get_stoi()\n",
    "\n",
    "itos_en = vocab_en.get_itos()\n",
    "stoi_en = vocab_en.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of token index in French (source) is 10001.\n",
      "The padded index in French (source) is 10000.\n",
      "The number of token index in English (target) is 10001.\n",
      "The padded index in English (target) is 10000.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(\"The number of token index in French (source) is {}.\".format(vocab_fr.__len__()))\n",
    "print(\"The padded index in French (source) is {}.\".format(stoi_fr[\"<pad>\"]))\n",
    "print(\"The number of token index in English (target) is {}.\".format(vocab_en.__len__()))\n",
    "print(\"The padded index in English (target) is {}.\".format(stoi_en[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a collator function, which is used for pre-processing in data loader.\n",
    "\n",
    "In this collator,\n",
    "\n",
    "(1) First we create a list of word's indices for source (French) and target (English) respectively as follows.\n",
    "\n",
    "```<start> this is pen <end>``` --> ```[2, 7, 5, 14, 1]```\n",
    "\n",
    "(2) For target (English) sequence, we separate into features (x) and labels (y).<br>\n",
    "In this task, we predict the next word in target (English) sequence using the current word's sequence (English) and the encoded context of source (French).<br>\n",
    "We then separate target sequence into the sequence iteself (x) and the following label (y).\n",
    "\n",
    "<u>before</u> :\n",
    "\n",
    "```[2, 7, 5, 14, 1]```\n",
    "\n",
    "<u>after</u> :\n",
    "\n",
    "```x : [2, 7, 5, 14, 1]```\n",
    "\n",
    "```y : [7, 5, 14, 1, -100]```\n",
    "\n",
    "> Note : Here I set -100 as an unknown label id, because PyTorch cross-entropy function (```torch.nn.functional.cross_entropy()```) has a property ```ignore_index``` which default value is -100.\n",
    "\n",
    "(3) Finally we pad the inputs (for both source and target) as follows.<br>\n",
    "The padded index in features is ```pad_index``` and the padded index in label is -100. (See above note.)\n",
    "\n",
    "```x : [2, 7, 5, 14, 1, N, ... , N]```\n",
    "\n",
    "```y : [7, 5, 14, 1, -100, -100, ... , -100]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seq_len_fr = 45\n",
    "seq_len_en = 38\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, feature_source_list, feature_target_list = [], [], []\n",
    "    for text_fr, text_en in batch:\n",
    "        # (1) tokenize to a list of word's indices\n",
    "        tokens_fr = vocab_fr(tokenizer(text_fr))\n",
    "        tokens_en = vocab_en(tokenizer(text_en))\n",
    "        # (2) separate into features and labels in target tokens (English)\n",
    "        y = tokens_en[1:]\n",
    "        y.append(-100)\n",
    "        # (3) limit length to seq_len and pad sequence\n",
    "        y = y[:seq_len_en]\n",
    "        tokens_fr = tokens_fr[:seq_len_fr]\n",
    "        tokens_en = tokens_en[:seq_len_en]\n",
    "        y += [-100] * (seq_len_en - len(y))\n",
    "        tokens_fr += [pad_index_fr] * (seq_len_fr - len(tokens_fr))\n",
    "        tokens_en += [pad_index_en] * (seq_len_en - len(tokens_en))\n",
    "        # add to list\n",
    "        label_list.append(y)\n",
    "        feature_source_list.append(tokens_fr)\n",
    "        feature_target_list.append(tokens_en)\n",
    "    # convert to tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
    "    feature_source_list = torch.tensor(feature_source_list, dtype=torch.int64).to(device)\n",
    "    feature_target_list = torch.tensor(feature_target_list, dtype=torch.int64).to(device)\n",
    "    return label_list, feature_source_list, feature_target_list\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    list(zip(train_data[:,0], train_data[:,1])),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape in batch : torch.Size([32, 38])\n",
      "feature source shape in batch : torch.Size([32, 45])\n",
      "feature target shape in batch : torch.Size([32, 38])\n",
      "***** label sample *****\n",
      "tensor([  14,  427,  164,   14,    8,  593,    1, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100], device='cuda:0')\n",
      "***** features (source) sample *****\n",
      "tensor([    2,    13,    39,   255,    53,    15,   641,     1, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000], device='cuda:0')\n",
      "***** features (target) sample *****\n",
      "tensor([    2,    14,   427,   164,    14,     8,   593,     1, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for labels, sources, targets in dataloader:\n",
    "    break\n",
    "\n",
    "print(\"label shape in batch : {}\".format(labels.size()))\n",
    "print(\"feature source shape in batch : {}\".format(sources.size()))\n",
    "print(\"feature target shape in batch : {}\".format(targets.size()))\n",
    "print(\"***** label sample *****\")\n",
    "print(labels[0])\n",
    "print(\"***** features (source) sample *****\")\n",
    "print(sources[0])\n",
    "print(\"***** features (target) sample *****\")\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In the formal algorithms of transformer, the set of embedded tokens are encoded by positions without any additional parameters.\n",
    "\n",
    "![Positional encoding](./images/transformer_positional_encoding.png)\n",
    "\n",
    "If there's no positional encoding, the sequence will be treated as a bag of tokens in neural networks. The positional information is needed for position-aware processing in attention.\n",
    "\n",
    "There exist several ways (variations) for positional encoding.<br>\n",
    "In this example, I'll apply the following positional encoding method (called **sinusoidal positional encoding**), which is introduced in the [original paper](https://arxiv.org/abs/1706.03762) of transformer.\n",
    "\n",
    "(1) The positional vector $PE(t)$ is :<br>\n",
    "$ PE(t,2i) = \\sin(t / 10000^{2i / d_e}) $<br>\n",
    "$ PE(t,2i+1) = \\cos(t / 10000^{2i / d_e}) $<br>\n",
    "for $0 \\leq i \\lt d_e / 2$<br>\n",
    "where $t = \\{0, 1, \\ldots\\}$ is position (time-step) and $d_e$ is embedding dimemsion.\n",
    "\n",
    "(2) For $t$-th token in the sequence, the embedding $E(t) \\in \\mathbb{R}^{d_e}$ then becomes $ E(t) + PE(t) $.\n",
    "\n",
    "> Note : Here I assume that $d_e$ (embedding dimension) is an even number. (When it's an odd number, the dimension between $E(t)$ and $PE(t)$ differs.)\n",
    "\n",
    "By applying this positional encoding, we can expect that the attention network will easily learn the position in each tokens, since there always exist a $ 2 \\times 2 $ matrix $\\mathbf{M}_{ik}$ (depending on $i$ and $k$) which satisfies\n",
    "\n",
    "$$ \\begin{pmatrix} PE(t+k,2i)\\\\PE(t+k,2i+1) \\end{pmatrix} = \\mathbf{M}_{ik} \\begin{pmatrix} PE(t,2i)\\\\PE(t,2i+1) \\end{pmatrix} $$\n",
    "\n",
    "for any $t$.\n",
    "\n",
    "> Note : In GPT, positional encoding is not a fixed encoding (not like above sinusoidal positional encoding) and it's also learned in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, embedding_dim):\n",
    "        assert(embedding_dim % 2 == 0)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1 / 10000^{2i / d_e}\n",
    "        #   --> (embedding_dim / 2, )\n",
    "        interval = 1.0 / (10000**(torch.arange(0, embedding_dim, 2.0) / embedding_dim))\n",
    "        # t\n",
    "        #   --> (seq_len, )\n",
    "        position = torch.arange(0, seq_len).float()\n",
    "        # t / 10000^{2i / d_e}\n",
    "        #   --> (seq_len, embedding_dim / 2)\n",
    "        radian = position[:, None] * interval[None, :]\n",
    "        # sin(t / 10000^{2i / d_e})\n",
    "        #   --> (seq_len, embedding_dim / 2, 1)\n",
    "        sin = torch.sin(radian).unsqueeze(dim=-1)\n",
    "        # cos(t / 10000^{2i / d_e})\n",
    "        #   --> (seq_len, embedding_dim / 2, 1)\n",
    "        cos = torch.cos(radian).unsqueeze(dim=-1)\n",
    "        # PE\n",
    "        #   --> (seq_len, embedding_dim / 2, 2)\n",
    "        pe_tmp = torch.concat((sin, cos), dim=-1)\n",
    "        # reshape\n",
    "        #   --> (seq_len, embedding_dim)\n",
    "        d = pe_tmp.size()[1]\n",
    "        self.pe = pe_tmp.view(-1, d * 2).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs + self.pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the output of positional encoding layer.<br>\n",
    "In the following example, positional vectors will become :\n",
    "\n",
    "```\n",
    "[\n",
    "  [sin(0), cos(0), sin(0/100), cos(0/100)],\n",
    "  [sin(1), cos(1), sin(1/100), cos(1/100)],\n",
    "  [sin(2), cos(2), sin(2/100), cos(2/100)],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### positional vector #####\n",
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998]], device='cuda:0')\n",
      "##### input vector #####\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]],\n",
      "\n",
      "        [[13., 14., 15., 16.],\n",
      "         [17., 18., 19., 20.],\n",
      "         [21., 22., 23., 24.]]], device='cuda:0')\n",
      "##### output #####\n",
      "tensor([[[ 1.0000,  3.0000,  3.0000,  5.0000],\n",
      "         [ 5.8415,  6.5403,  7.0100,  9.0000],\n",
      "         [ 9.9093,  9.5839, 11.0200, 12.9998]],\n",
      "\n",
      "        [[13.0000, 15.0000, 15.0000, 17.0000],\n",
      "         [17.8415, 18.5403, 19.0100, 21.0000],\n",
      "         [21.9093, 21.5839, 23.0200, 24.9998]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test = PositionalEncoding(3, 4).to(device)\n",
    "print(\"##### positional vector #####\")\n",
    "print(test.pe)\n",
    "# The input size should be (batch_size, seq_len, embedding_dim)\n",
    "x = torch.arange(1, 25).float()\n",
    "x = x.view(2, 3, 4).to(device)\n",
    "print(\"##### input vector #####\")\n",
    "print(x)\n",
    "y = test(x)\n",
    "print(\"##### output #####\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multi-head and Scaled Dot-Product Attention\n",
    "\n",
    "Next I'll implement attention layer as follows.<br>\n",
    "For the purpose of your learning, I'll implement the scaled dot-product attention layer from scratch.\n",
    "\n",
    "> Note : In PyTorch, you can use built-in ```torch.nn.MultiheadAttention```.\n",
    "\n",
    "In 3 parts of attention (encoder's self-attention, decoder's self-attention, and encoder-decoder cross attention), it runs the following steps. (See above description for the semantics of this model.) :\n",
    "\n",
    "(1) The embedded inputs in the sequence are processed by dense networks (fully-connected feed-forward networks), and \"query\" ($Q$), \"key\" ($K$), and \"value\" ($V$) are then generated.\n",
    "\n",
    "(2) Compute the relationship score between $Q$ and $K$ by the dot product, $Q \\cdot K^T$.\n",
    "\n",
    "(3) Scale the score by multiplying $\\frac{1}{\\sqrt{d}}$, where $d$ is the number of attention dimension.\n",
    "\n",
    "(4) In decoder side, apply causal mask.<br>\n",
    "Later I'll explain details about this optional causal mask ...\n",
    "\n",
    "(5) The relationship between $Q$ and $K$ are softmaxed, i.e, normalized by $\\displaystyle \\frac{e^{s_i}}{e^{s_0} + e^{s_1} + \\cdots + e^{s_{t-1}}}$ where $(s_0, s_1, \\ldots , s_{t-1})$ is the relationship vector and $t$ is time-step.<br>\n",
    "I note that the above scaling by $\\frac{1}{\\sqrt{d}}$ works as a softmax temprature in this step.\n",
    "\n",
    "(6) Finally the result (softmaxed score) is performed by the dot product with $V$.<br>\n",
    "The final result is then $\\displaystyle \\verb|softmax| \\left( \\frac{Q \\cdot K^T}{\\sqrt{d}} \\right) \\cdot V$.\n",
    "\n",
    "As I have mentioned above, input1 and input2 in the following picture will be the same in ecoder's and decoder's self-attention parts.\n",
    "\n",
    "![Multi-head attention](./images/transformer_attention.png)\n",
    "\n",
    "To make multiple attention work in parallel, model dimension (here 256) is divided into multiple heads (here 8), and each head will then have ```model_dim / head_num``` dimension (here 32). Finally, these separated heads are concatenated and then applied dense network to obtain model dimension's result. (See above picture.)<br>\n",
    "This technique will make our model have rich expression without losing the computing costs.\n",
    "\n",
    "Now it's time to explain about causal mask.<br>\n",
    "In self-attention on decoder's side (when ```use_causal_mask=True``` in the following code), each token only refers past tokens and cannot access to the future tokens. (See the following picture.)\n",
    "\n",
    "![Causal reference](images/transformer_causal_reference.png)\n",
    "\n",
    "For this reason, the softmax operation in decoder's self-attention is performed only on lower triangular matrix as follows.<br>\n",
    "This is because we apply optional mask before softmax operation.\n",
    "\n",
    "![Causal attention in decoder](images/transformer_causal_attention.png)\n",
    "\n",
    "> Note : To make a lower triangular matrix, here I use ```torch.tril()``` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MyMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Initializes a MyMultiHeadAttention module.\n",
    "\n",
    "    Args:\n",
    "        embedding_dim (int): The number of embedding dimensiom\n",
    "        attention_dim (int): The number of dimension within attention unit\n",
    "        num_heads (int): The number of the divided heads (See above.)\n",
    "        use_causal_mask (bool): Whether to mask the future tokens (See above.)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        attention_dim,\n",
    "        num_heads,\n",
    "        use_causal_mask=False):\n",
    "\n",
    "        assert(attention_dim % num_heads == 0)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.head_dim = int(attention_dim / num_heads)\n",
    "        self.use_causal_mask = use_causal_mask\n",
    "\n",
    "        self.q_layer = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.k_layer = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.v_layer = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.output_linear = nn.Linear(attention_dim, attention_dim, bias=False)\n",
    "\n",
    "    \"\"\"\n",
    "    When self-attention, input2 will be None\n",
    "    \"\"\"\n",
    "    def forward(self, input1, mask1, input2=None, mask2=None):\n",
    "        if input2 is None:\n",
    "            input2 = input1\n",
    "        if mask2 is None:\n",
    "            mask2 = mask1\n",
    "\n",
    "        # get size\n",
    "        seq_len1 = input1.size()[1]\n",
    "        seq_len2 = input2.size()[1]\n",
    "\n",
    "        # apply query/key/value net - see above 1\n",
    "        #   --> (batch_size, seq_len, attention_dim)\n",
    "        q = self.q_layer(input1)\n",
    "        k = self.k_layer(input2)\n",
    "        v = self.v_layer(input2)\n",
    "\n",
    "        # divide into multiple heads :\n",
    "        #   --> (batch_size, seq_len, num_heads, attention_dim / num_heads)\n",
    "        q = q.view(-1, seq_len1, self.num_heads, self.head_dim)\n",
    "        k = k.view(-1, seq_len2, self.num_heads, self.head_dim)\n",
    "        v = v.view(-1, seq_len2, self.num_heads, self.head_dim)\n",
    "\n",
    "        # compute Q K^T - see above 2\n",
    "        #   --> (batch_size, seq_len1, seq_len2, num_heads)\n",
    "        score = torch.einsum(\"bihd,bjhd->bijh\", q, k)\n",
    "        \n",
    "        # scale the result by 1/sqrt(d) - see above 3\n",
    "        #   --> (batch_size, seq_len1, seq_len2, num_heads)\n",
    "        score = score / self.head_dim**0.5\n",
    "\n",
    "        # generate causal mask matrix - see above 4\n",
    "        # (for decoder's self-attention only)\n",
    "        #   --> (seq_len1, seq_len2)\n",
    "        causal_mask = torch.ones(seq_len1, seq_len2).int().to(device)\n",
    "        if self.use_causal_mask:\n",
    "            # when applying causal mask, the shape of input1 and input2 should be same\n",
    "            assert(seq_len1 == seq_len2)\n",
    "            causal_mask = torch.tril(causal_mask)\n",
    "\n",
    "        # generate sequence mask matrix\n",
    "        #   --> (batch_size, seq_len1, 1) @ (batch_size, 1, seq_len2) = (batch_size, seq_len1, seq_len2)\n",
    "        # (note : bmm should be used for TensorFloat32. Here I then use torch.einsum() instead.)\n",
    "        seq_mask = torch.einsum(\n",
    "            \"bxt,bty->bxy\",\n",
    "            mask1.unsqueeze(dim=2),\n",
    "            mask2.unsqueeze(dim=1))\n",
    "        # seq_mask = mask1.unsqueeze(dim=2) @ mask2.unsqueeze(dim=1)\n",
    "\n",
    "        # generate final mask matrix\n",
    "        #   --> (batch_size, seq_len1, seq_len2)\n",
    "        mask = causal_mask * seq_mask\n",
    "        #   --> (batch_size, seq_len1, seq_len2, 1)\n",
    "        mask = mask.unsqueeze(dim=3)\n",
    "        #   --> (batch_size, seq_len1, seq_len2, num_heads)\n",
    "        mask = mask.expand(-1, -1, -1, self.num_heads)\n",
    "\n",
    "        # apply softmax with mask - see above 5\n",
    "        #   --> (batch_size, seq_len1, seq_len2, num_heads)\n",
    "        score = score.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        score = F.softmax(score, dim=2)\n",
    "        # values in input1's padded position will become \"nan\" by\n",
    "        # softmax operation, because it's divided by zero.\n",
    "        score = score.nan_to_num()\n",
    "\n",
    "        # dot product with V - see above 6\n",
    "        #   --> (batch_size, seq_len1, num_heads, attention_dim / num_heads)\n",
    "        out = torch.einsum(\"bijh,bjhd->bihd\", score, v)\n",
    "\n",
    "        # concatenate all heads and apply linear\n",
    "        #   --> (batch_size, seq_len1, attention_dim)\n",
    "        out = out.reshape(-1, seq_len1, self.attention_dim)\n",
    "        #   --> (batch_size, seq_len1, attention_dim)\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Now let's implement encoder side using previously generated multi-head attention module.<br>\n",
    "Same as [exercise 08](./08_attention.ipynb), the purpose of encoder is to generate the context of source sequence (French text). However, unlike exercise 08, we don't use RNN (GRU) and apply the scaled dot-product attention (self-attention) instead.\n",
    "\n",
    "As you can see in below picture (as it shows with \"Nx\"), the encoder in transformer is multi-layered architecture, in which it has the repeated layers.<br>\n",
    "For this reason, we'll first implement the following repeatable single layer of component.\n",
    "\n",
    "![Encoding Layer](./images/transformer_encoding_layer.png)\n",
    "\n",
    "> Note : As you can see above, it adds an output of identity layer (which is written by \"Add&Norm\" in above picture) in the end of each layers. This is a known technique called **residual learning** in order to address a degradation problem of training accuracy in deep networks. (See \"[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\" (He, et al., 2015) for details.)<br>\n",
    "> ![Residual Network](./images/transformer_residual01.png)<br>\n",
    "> Today, a lot of transformers place the layer normalization between the residual blocks as follows. (In this example, I'll create the code implementation accompanying the original paper.)<br>\n",
    "> ![Residual Network](./images/transformer_residual02.png)\n",
    "\n",
    "> Note : I have also added a dropout for regularization in the following code.<br>\n",
    "> For the effect of a dropout, please see [here](https://tsmatz.wordpress.com/2017/09/13/overfitting-for-regression-and-deep-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleEncodingLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MyMultiHeadAttention(\n",
    "            embedding_dim=model_dim,\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(model_dim, eps=0.001)\n",
    "        self.output_dense1 = nn.Linear(model_dim, hidden_dim)\n",
    "        self.output_dense2 = nn.Linear(hidden_dim, model_dim)\n",
    "\n",
    "    def forward(self, inputs, masks):\n",
    "        # apply self-attention\n",
    "        attention_outputs = self.self_attention(inputs, masks)\n",
    "        # add & layer norm (with dropout)\n",
    "        attention_outputs = F.dropout(attention_outputs, p=0.1)\n",
    "        attention_outputs = attention_outputs + inputs\n",
    "        attention_outputs = self.norm(attention_outputs)\n",
    "        # feed forward\n",
    "        linear_outputs = self.output_dense1(attention_outputs)\n",
    "        linear_outputs = F.relu(linear_outputs)\n",
    "        linear_outputs = self.output_dense2(linear_outputs)\n",
    "        # add & layer norm (with dropout)\n",
    "        linear_outputs = F.dropout(linear_outputs, p=0.1)\n",
    "        linear_outputs = linear_outputs + attention_outputs\n",
    "        linear_outputs = self.norm(linear_outputs)\n",
    "\n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With previously generated component (```SingleEncodingLayer```), now we implement the multi-layered encoder as follows.\n",
    "\n",
    "![Encoding Layer](./images/transformer_encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, padding_idx, model_dim, num_layers, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            model_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=model_dim,\n",
    "        )\n",
    "        self.encoding_layers = nn.ModuleList([\n",
    "            SingleEncodingLayer(\n",
    "                model_dim=model_dim,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "            )\n",
    "            for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # get mask\n",
    "        masks = (inputs != self.padding_idx).int()\n",
    "        # apply embedding\n",
    "        outputs = self.embedding(inputs)\n",
    "        # apply positional encoding\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        # apply multi-layered encoders\n",
    "        for enc_layer in self.encoding_layers:\n",
    "            outputs = enc_layer(outputs, masks)\n",
    "\n",
    "        return outputs, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Next implement decoder side.<br>\n",
    "Same as encoder, we'll first implement a repeatable single layer component as the following picture shows.\n",
    "\n",
    "![Decoding Layer](./images/transformer_decoding_layer.png)\n",
    "\n",
    "Unlike encoder, both self-attention and cross-attention are applied in decoder. (See above.)\n",
    "\n",
    "In the first attention, the target sequence (English) is encoded by self-attention. As I have mentioned above, causal masking is applied in this decoder's self-attention. (Set the property ```use_causal_mask=True``` in our custom attention.)<br>\n",
    "\n",
    "The next attention (cross-attention) is for machine translation. Same as [exercise 08](./08_attention.ipynb), both the encoder's outputs (contexts) for source (French) and the current decoded results for target (English) are fed into this attention, and the attended results between decoder's inputs and encoder's outputs will be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDecodingLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MyMultiHeadAttention(\n",
    "            embedding_dim=model_dim,\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "            use_causal_mask=True,\n",
    "        )\n",
    "        self.cross_attention = MyMultiHeadAttention(\n",
    "            embedding_dim=model_dim,\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(model_dim, eps=0.001)\n",
    "        self.output_dense1 = nn.Linear(model_dim, hidden_dim)\n",
    "        self.output_dense2 = nn.Linear(hidden_dim, model_dim)\n",
    "\n",
    "    def forward(self, inputs, masks, enc_outputs, enc_masks):\n",
    "        # self-attention with causal masking\n",
    "        attention_outputs = self.self_attention(inputs, masks)\n",
    "        # add & layer norm (with dropout)\n",
    "        attention_outputs = F.dropout(attention_outputs, p=0.1)\n",
    "        attention_outputs = attention_outputs + inputs\n",
    "        attention_outputs = self.norm(attention_outputs)\n",
    "        # encoder-decoder attention\n",
    "        scored_outputs = self.cross_attention(\n",
    "            input1=attention_outputs,\n",
    "            mask1=masks,\n",
    "            input2=enc_outputs,\n",
    "            mask2=enc_masks,\n",
    "        )\n",
    "        # add & layer norm (with dropout)\n",
    "        scored_outputs = F.dropout(scored_outputs, p=0.1)\n",
    "        scored_outputs = scored_outputs + attention_outputs\n",
    "        scored_outputs = self.norm(scored_outputs)\n",
    "        # feed forward\n",
    "        linear_outputs = self.output_dense1(scored_outputs)\n",
    "        linear_outputs = F.relu(linear_outputs)\n",
    "        linear_outputs = self.output_dense2(linear_outputs)\n",
    "        # add & layer norm (with dropout)\n",
    "        linear_outputs = F.dropout(linear_outputs, p=0.1)\n",
    "        linear_outputs = linear_outputs + scored_outputs\n",
    "        linear_outputs = self.norm(linear_outputs)\n",
    "\n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build multi-layered decoder with previous layer component (```SingleDecodingLayer```).\n",
    "\n",
    "![Decoder](./images/transformer_decoder.png)\n",
    "\n",
    "The outputs is used for predicting the next vocabulary by one-hot outputs, and the output's shape will then be ```(batch_size, sequence_length, vocabulary_size)```.\n",
    "\n",
    "> Note : In this example, the final softmax will be applied in loss computation, and here I don't explicitly implement this operation in this module. (The decoder will then output logits, not probabilities.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, padding_idx, model_dim, num_layers, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            model_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=model_dim,\n",
    "        )\n",
    "        self.decoding_layers = nn.ModuleList([\n",
    "            SingleDecodingLayer(\n",
    "                model_dim=model_dim,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "            )\n",
    "            for _ in range(num_layers)])\n",
    "        self.output_dense = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_inputs, enc_outputs, enc_masks):\n",
    "        # get mask\n",
    "        target_masks = (target_inputs != self.padding_idx).int()\n",
    "        # apply embedding\n",
    "        outputs = self.embedding(target_inputs)\n",
    "        # apply positional encoding\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        # apply multi-layered decoders\n",
    "        for dec_layer in self.decoding_layers:\n",
    "            outputs = dec_layer(\n",
    "                inputs=outputs,\n",
    "                masks=target_masks,\n",
    "                enc_outputs=enc_outputs,\n",
    "                enc_masks=enc_masks,\n",
    "            )\n",
    "        # apply final Linear\n",
    "        #   (batch_size, seq_len, model_dim) --> (batch_size, seq_len, vocab_size)\n",
    "        outputs = self.output_dense(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (Transformer)\n",
    "\n",
    "Using previous models, now we build the training loop.\n",
    "\n",
    "First we set the following parameters for training.<br>\n",
    "In this example, the training dataset consists of single sentences (not long text) and I have reduced parameters to speed up the training, compared with the parameters which is used in the original paper.\n",
    "\n",
    "> Note : By increasing parameters, transformers will have the ability to capture more difficult contexts, but it'll need more training and corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "hidden_dim = 1024\n",
    "\n",
    "### In the original paper, the following parameters are used.\n",
    "#model_dim = 512\n",
    "#num_heads = 8\n",
    "#num_layers = 6\n",
    "#hidden_dim = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the following learning rate scheduler is used for this model, and we also apply this scheduling in this training.<br>\n",
    "The following function is used to modify learning rate in the training.\n",
    "\n",
    "$$ \\verb|lrate| = d^{-0.5}_{\\verb|model|} \\cdot \\min(\\verb|step_num|^{-0.5},\\verb|step_num|\\cdot\\verb|warmup_steps|^{-1.5}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step, model_dim, warmup_steps=4000):\n",
    "    step = float(step)\n",
    "    model_dim = float(model_dim)\n",
    "    if step == 0.0:\n",
    "        val1 = 0.0\n",
    "    else:\n",
    "        val1 = 1.0 / (step ** 0.5)\n",
    "    val2 = step / (warmup_steps ** 1.5)\n",
    "    return min(val1,val2) / (model_dim ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, put it all together and run training as follows.\n",
    "\n",
    "In below code, the loss on label id=-100 is ignored in ```cross_entropy()``` function. The padded position and the end of sequence will then be ignored in optimization.\n",
    "\n",
    "> Note : Because the default value of  ```ignore_index``` property in ```cross_entropy()``` function is -100. (You can change this default value.)\n",
    "\n",
    "Transformer has ability to capture complex contexts, but I note that here I just simply apply training by using primitive data of single sentence in small epochs.<br>\n",
    "Please try more large and complex data by adjusting above parameters and the number of training epochs.\n",
    "\n",
    "You will also find that transformer is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 2.1720 - accuracy: 0.6667\n",
      "Epoch 2 - loss: 1.8233 - accuracy: 0.7147\n",
      "Epoch 3 - loss: 1.5693 - accuracy: 0.7454\n",
      "Epoch 4 - loss: 1.3702 - accuracy: 0.7500\n",
      "Epoch 5 - loss: 1.1836 - accuracy: 0.7729\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "enc_model = Encoder(\n",
    "    vocab_size=vocab_fr.__len__(),\n",
    "    seq_len=seq_len_fr,\n",
    "    padding_idx=pad_index_fr,\n",
    "    model_dim=model_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    ").to(device)\n",
    "dec_model = Decoder(\n",
    "    vocab_size=vocab_en.__len__(),\n",
    "    seq_len=seq_len_en,\n",
    "    padding_idx=pad_index_en,\n",
    "    model_dim=model_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    ").to(device)\n",
    "\n",
    "all_params = list(enc_model.parameters()) + list(dec_model.parameters())\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=all_params,\n",
    "    lr=get_lr(0, model_dim),\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    ")\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, sources, targets in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        enc_outputs, enc_masks = enc_model(sources)\n",
    "        logits = dec_model(targets, enc_outputs, enc_masks)\n",
    "        loss = F.cross_entropy(logits.transpose(1,2), labels)\n",
    "        loss.backward()\n",
    "        # update learning rate and step\n",
    "        lr = get_lr(step, model_dim)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=2)\n",
    "        num_correct = (pred_labels == labels).float().sum()\n",
    "        num_total = (labels != -100).float().sum()\n",
    "        accuracy = num_correct / num_total\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Text\n",
    "\n",
    "Now translate French text to English text with trained model. (All these sentences are not in training set.)\n",
    "\n",
    "Here I simply translate several brief sentences, but the metrics to evaluate text-generation task will not be so easy. (Because simply checking an exact match to a reference text is not optimal.)<br>\n",
    "To eveluate the trained model, use some common metrics available in text generation, such as, BLEU or ROUGE.\n",
    "\n",
    "> Note : Here I use greedy search and this will sometimes lead to wrong sequence. For drawbacks and solutinos, see note in [this example](./05_language_model_basic.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "end_index_en = stoi_en[\"<end>\"]\n",
    "max_output = 128\n",
    "\n",
    "def translate(sentence):\n",
    "    # preprocess inputs\n",
    "    text_fr = sentence\n",
    "    text_fr = text_fr.lower()\n",
    "    text_fr = \" \".join([\"<start>\", text_fr, \"<end>\"])\n",
    "    text_en_list = [\"<start>\"]\n",
    "    text_en = \" \".join(text_en_list)\n",
    "    _, tokens_fr, tokens_en = collate_batch(list(zip([text_fr], [text_en])))\n",
    "\n",
    "    # process encoder\n",
    "    enc_outputs, enc_masks = enc_model(tokens_fr)\n",
    "\n",
    "    # process decoder\n",
    "    for loop in range(max_output):\n",
    "        logits = dec_model(\n",
    "            tokens_en,\n",
    "            enc_outputs,\n",
    "            enc_masks,\n",
    "        )\n",
    "        idx_en = logits[0][len(text_en_list) - 1].argmax()\n",
    "        next_word_en = itos_en[idx_en]\n",
    "        text_en_list.append(next_word_en)\n",
    "        if idx_en.item() == end_index_en:\n",
    "            break\n",
    "        text_en = \" \".join(text_en_list)\n",
    "        _, _, tokens_en = collate_batch(list(zip([text_fr], [text_en])))\n",
    "\n",
    "    return \" \".join(text_en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love the guitar <end>\n",
      "<start> he lives in japan <end>\n",
      "<start> this pen is used to him <end>\n",
      "<start> this is my favorite song <end>\n",
      "<start> he drives a car and go to new york <end>\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"j'aime la guitare\")) # i like guitar\n",
    "print(translate(\"il vit au japon\")) # he lives in Japan\n",
    "print(translate(\"ce stylo est utilisé par lui\")) # this pen is used by him\n",
    "print(translate(\"c'est ma chanson préférée\")) # that's my favorite song\n",
    "print(translate(\"il conduit une voiture et va à new york\")) # he drives a car and goes to new york"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
