{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model - Basic (Word Prediction Example)\n",
    "\n",
    "In this example, I'll show an example of simple language model.<br>\n",
    "In general, the language model is used for a variety of NLP tasks, such as, translation, transcription, summarization, etc. In this example, however, we just train and use the model for text generation (next word prediction).<br>\n",
    "Unlike [previous example](./02_custom_embedding.ipynb), these models will recognize the order of words in the sequence.\n",
    "\n",
    "RNN-based specialized architecture (such as, LSTM or Transformer) is widely used to train language model in today's algrithms. But, in this example, I'll briefly apply only DenseNet (feed-forward network) for the purpose of your beginning. (In the later examples, we'll discuss more practical models with RNN.)<br>\n",
    "See the following diagram for entire network.\n",
    "\n",
    "![Model in this exercise](images/language_model_beginning.png?raw=true)\n",
    "\n",
    "Thereby, I note that this model won't care the past context.<br>\n",
    "For example, even when the following sentence is given, \n",
    "\n",
    "\"In the United States, the president has now been\"\n",
    "\n",
    "it won't care the context \"In the United States\" when it refers the last 5 words in the network. (It might then predict the incorrect word in this context and the accuracy won't also be so high in this example. See [later example](./06_language_model_rnn.ipynb) for RNN-based architecture, which will address this problem.)\n",
    "\n",
    "Nevertheless, the neural language models will be well-generalized more than traditional statistical models for unseen data. For instance, if \"red shirt\" and \"blud shirt\" occurs in training set, \"green shirt\" (which is not seen in training set) will also be predicted by the trained neural model, because the model knows that \"red\", \"blue\", and \"green\" occur in the same context.\n",
    "\n",
    "The language model in this example can be treated as unsupervised approach.<br>\n",
    "As you saw in [previous example](./02_custom_embedding.ipynb), the word embedding is also a byproduct in generated language model.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/nlp-tutorials/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.6.2 pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I have used short description text in news papers, since it's formal-styled concise sentence. (It's today's modern English, not including slangs.)<br>\n",
    "Before starting, please download [News_Category_Dataset_v2.json](https://www.kaggle.com/datasets/rmisra/news-category-dataset) (collected by HuffPost) in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         She left her husband. He killed their children...\n",
       "1                                  Of course it has a song.\n",
       "2         The actor and his longtime girlfriend Anna Ebe...\n",
       "3         The actor gives Dems an ass-kicking for not fi...\n",
       "4         The \"Dietland\" actress said using the bags is ...\n",
       "                                ...                        \n",
       "200848    Verizon Wireless and AT&T are already promotin...\n",
       "200849    Afterward, Azarenka, more effusive with the pr...\n",
       "200850    Leading up to Super Bowl XLVI, the most talked...\n",
       "200851    CORRECTION: An earlier version of this story i...\n",
       "200852    The five-time all-star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"News_Category_Dataset_v2.json\",lines=True)\n",
    "train_data = df[\"short_description\"]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), we standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation except \"'\" (e.g, Ken's bag) and \"&\" (e.g, AT&T)\n",
    "\n",
    "> Note : N-gram words (such as, \"New York\", \"ice cream\") should also be dealed with, but here I have skipped these pre-processing to make simplify.<br>\n",
    "> In the strict pre-processing, we should also care about the polysemy. (The different meanings in the same word should have different tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         she left her husband he killed their children ...\n",
       "1                                   of course it has a song\n",
       "2         the actor and his longtime girlfriend anna ebe...\n",
       "3         the actor gives dems an ass kicking for not fi...\n",
       "4         the dietland actress said using the bags is a ...\n",
       "                                ...                        \n",
       "200848    verizon wireless and at&t are already promotin...\n",
       "200849    afterward azarenka more effusive with the pres...\n",
       "200850    leading up to super bowl xlvi the most talked ...\n",
       "200851    correction an earlier version of this story in...\n",
       "200852    the five time all star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.str.lower()\n",
    "train_data = train_data.str.replace(\"-\",\" \")\n",
    "train_data = train_data.str.replace(\"[^'\\&\\w\\s]\",\"\")\n",
    "train_data = train_data.str.strip()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequence inputs\n",
    "\n",
    "Same as in [previous example](02_custom_embedding.ipynb), we will generate the sequence of word's indices (i.e, tokenize) from text.\n",
    "\n",
    "![Index vectorize](images/index_vectorize.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_word = 70000\n",
    "\n",
    "corpus = \" \".join(train_data)\n",
    "new_tokens = [w for w in corpus.split() if w.isalpha()]\n",
    "new_corpus = \" \".join(new_tokens)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=max_word,\n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "tokenizer.fit_on_texts([new_corpus])\n",
    "#vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, each sentence is separated into 5 preceding word's sequence and word label (total 6 words in each sequence) as follows.\n",
    "\n",
    "![Separate words](images/separate_sequence_for_next_words.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training input sequence :3266478\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_len = 5 + 1\n",
    "input_seq = []\n",
    "for s in train_data:\n",
    "    token_list = tokenizer.texts_to_sequences([s])[0]\n",
    "    # add termination index 0\n",
    "    token_list.append(0)\n",
    "    for i in range(seq_len, len(token_list) + 1):\n",
    "        seq_list = token_list[i-seq_len:i]\n",
    "        input_seq.append(seq_list)\n",
    "print(\"The number of training input sequence :{}\".format(len(input_seq)))\n",
    "input_seq = np.array(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = input_seq[:,:-1], input_seq[:,-1]\n",
    "train_tf_data = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "def to_one_hot(x, y):\n",
    "   return x, tf.one_hot(y, depth=max_word)\n",
    "train_tf_data = train_tf_data.map(lambda x, y: to_one_hot(x, y))\n",
    "#tf.data.experimental.save(train_tf_data, \"saved_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tf_data = tf.data.experimental.load(\"saved_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(\n",
    "    max_word,\n",
    "    embedding_dim,\n",
    "    input_length=seq_len - 1,\n",
    "    trainable=True,\n",
    "    name=\"embedding\"))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    256,\n",
    "    activation=\"relu\",\n",
    "    trainable=True))\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    78,\n",
    "    activation=\"relu\",\n",
    "    trainable=True))\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    max_word,\n",
    "    activation=None,\n",
    "    trainable=True))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "6380/6380 [==============================] - 1142s 179ms/step - loss: 6.3752 - accuracy: 0.1221\n",
      "Epoch 2/60\n",
      "6380/6380 [==============================] - 1134s 178ms/step - loss: 5.7689 - accuracy: 0.1558\n",
      "Epoch 3/60\n",
      "6380/6380 [==============================] - 1138s 178ms/step - loss: 5.5038 - accuracy: 0.1683\n",
      "Epoch 4/60\n",
      "6380/6380 [==============================] - 1137s 178ms/step - loss: 5.3322 - accuracy: 0.1765\n",
      "Epoch 5/60\n",
      "6380/6380 [==============================] - 1139s 178ms/step - loss: 5.2091 - accuracy: 0.1827\n",
      "Epoch 6/60\n",
      "6380/6380 [==============================] - 1139s 179ms/step - loss: 5.1146 - accuracy: 0.1876\n",
      "Epoch 7/60\n",
      "6380/6380 [==============================] - 1143s 179ms/step - loss: 5.0395 - accuracy: 0.1917\n",
      "Epoch 8/60\n",
      "6380/6380 [==============================] - 1139s 178ms/step - loss: 4.9769 - accuracy: 0.1955\n",
      "Epoch 9/60\n",
      "6380/6380 [==============================] - 1128s 177ms/step - loss: 4.9252 - accuracy: 0.1988\n",
      "Epoch 10/60\n",
      "6380/6380 [==============================] - 1130s 177ms/step - loss: 4.8809 - accuracy: 0.2024\n",
      "Epoch 11/60\n",
      "6380/6380 [==============================] - 1142s 179ms/step - loss: 4.8429 - accuracy: 0.2058\n",
      "Epoch 12/60\n",
      "6380/6380 [==============================] - 1140s 179ms/step - loss: 4.8100 - accuracy: 0.2090\n",
      "Epoch 13/60\n",
      "6380/6380 [==============================] - 1134s 178ms/step - loss: 4.7812 - accuracy: 0.2119\n",
      "Epoch 14/60\n",
      "6380/6380 [==============================] - 1137s 178ms/step - loss: 4.7567 - accuracy: 0.2145\n",
      "Epoch 15/60\n",
      "6380/6380 [==============================] - 1140s 179ms/step - loss: 4.7345 - accuracy: 0.2168\n",
      "Epoch 16/60\n",
      "6380/6380 [==============================] - 1140s 179ms/step - loss: 4.7148 - accuracy: 0.2189\n",
      "Epoch 17/60\n",
      "6380/6380 [==============================] - 1138s 178ms/step - loss: 4.6968 - accuracy: 0.2210\n",
      "Epoch 18/60\n",
      "6380/6380 [==============================] - 1138s 178ms/step - loss: 4.6804 - accuracy: 0.2230\n",
      "Epoch 19/60\n",
      "6380/6380 [==============================] - 1137s 178ms/step - loss: 4.6656 - accuracy: 0.2246\n",
      "Epoch 20/60\n",
      "6380/6380 [==============================] - 1148s 180ms/step - loss: 4.6520 - accuracy: 0.2261\n",
      "Epoch 21/60\n",
      "6380/6380 [==============================] - 1138s 178ms/step - loss: 4.6395 - accuracy: 0.2275\n",
      "Epoch 22/60\n",
      "6380/6380 [==============================] - 1133s 178ms/step - loss: 4.6280 - accuracy: 0.2289\n",
      "Epoch 23/60\n",
      "6380/6380 [==============================] - 1136s 178ms/step - loss: 4.6171 - accuracy: 0.2302\n",
      "Epoch 24/60\n",
      "6380/6380 [==============================] - 1148s 180ms/step - loss: 4.6073 - accuracy: 0.2314\n",
      "Epoch 25/60\n",
      "6380/6380 [==============================] - 1139s 178ms/step - loss: 4.5978 - accuracy: 0.2325\n",
      "Epoch 26/60\n",
      "6380/6380 [==============================] - 1140s 179ms/step - loss: 4.5889 - accuracy: 0.2337\n",
      "Epoch 27/60\n",
      "6380/6380 [==============================] - 1132s 177ms/step - loss: 4.5811 - accuracy: 0.2347\n",
      "Epoch 28/60\n",
      "6380/6380 [==============================] - 1127s 177ms/step - loss: 4.5734 - accuracy: 0.2355\n",
      "Epoch 29/60\n",
      "6380/6380 [==============================] - 1132s 177ms/step - loss: 4.5662 - accuracy: 0.2365\n",
      "Epoch 30/60\n",
      "6380/6380 [==============================] - 1143s 179ms/step - loss: 4.5594 - accuracy: 0.2374\n",
      "Epoch 31/60\n",
      "6380/6380 [==============================] - 1137s 178ms/step - loss: 4.5532 - accuracy: 0.2380\n",
      "Epoch 32/60\n",
      "6380/6380 [==============================] - 1142s 179ms/step - loss: 4.5471 - accuracy: 0.2388\n",
      "Epoch 33/60\n",
      "6380/6380 [==============================] - 1135s 178ms/step - loss: 4.5411 - accuracy: 0.2395\n",
      "Epoch 34/60\n",
      "6380/6380 [==============================] - 1125s 176ms/step - loss: 4.5361 - accuracy: 0.2401\n",
      "Epoch 35/60\n",
      "6380/6380 [==============================] - 1134s 178ms/step - loss: 4.5310 - accuracy: 0.2407\n",
      "Epoch 36/60\n",
      "6380/6380 [==============================] - 1132s 177ms/step - loss: 4.5258 - accuracy: 0.2415\n",
      "Epoch 37/60\n",
      "6380/6380 [==============================] - 1131s 177ms/step - loss: 4.5210 - accuracy: 0.2421\n",
      "Epoch 38/60\n",
      "6380/6380 [==============================] - 1139s 178ms/step - loss: 4.5170 - accuracy: 0.2426\n",
      "Epoch 39/60\n",
      "6380/6380 [==============================] - 1138s 178ms/step - loss: 4.5128 - accuracy: 0.2430\n",
      "Epoch 40/60\n",
      "6380/6380 [==============================] - 1137s 178ms/step - loss: 4.5088 - accuracy: 0.2435\n",
      "Epoch 41/60\n",
      "6380/6380 [==============================] - 1135s 178ms/step - loss: 4.5048 - accuracy: 0.2442\n",
      "Epoch 42/60\n",
      "6380/6380 [==============================] - 1132s 177ms/step - loss: 4.5016 - accuracy: 0.2444\n",
      "Epoch 43/60\n",
      "6380/6380 [==============================] - 1132s 177ms/step - loss: 4.4986 - accuracy: 0.2449\n",
      "Epoch 44/60\n",
      "6380/6380 [==============================] - 1129s 177ms/step - loss: 4.4949 - accuracy: 0.2453\n",
      "Epoch 45/60\n",
      "6380/6380 [==============================] - 1132s 177ms/step - loss: 4.4915 - accuracy: 0.2457\n",
      "Epoch 46/60\n",
      "6380/6380 [==============================] - 1134s 178ms/step - loss: 4.4888 - accuracy: 0.2460\n",
      "Epoch 47/60\n",
      "6380/6380 [==============================] - 1132s 177ms/step - loss: 4.4858 - accuracy: 0.2464\n",
      "Epoch 48/60\n",
      "6380/6380 [==============================] - 1134s 178ms/step - loss: 4.4833 - accuracy: 0.2468\n",
      "Epoch 49/60\n",
      "6380/6380 [==============================] - 1134s 178ms/step - loss: 4.4809 - accuracy: 0.2471\n",
      "Epoch 50/60\n",
      "6380/6380 [==============================] - 1140s 179ms/step - loss: 4.4779 - accuracy: 0.2476\n",
      "Epoch 51/60\n",
      "6380/6380 [==============================] - 1134s 178ms/step - loss: 4.4758 - accuracy: 0.2476\n",
      "Epoch 52/60\n",
      "6380/6380 [==============================] - 1139s 178ms/step - loss: 4.4733 - accuracy: 0.2481\n",
      "Epoch 53/60\n",
      "6380/6380 [==============================] - 1134s 178ms/step - loss: 4.4714 - accuracy: 0.2483\n",
      "Epoch 54/60\n",
      "6380/6380 [==============================] - 1132s 177ms/step - loss: 4.4689 - accuracy: 0.2485\n",
      "Epoch 55/60\n",
      "6380/6380 [==============================] - 1137s 178ms/step - loss: 4.4674 - accuracy: 0.2488\n",
      "Epoch 56/60\n",
      "6380/6380 [==============================] - 1140s 179ms/step - loss: 4.4651 - accuracy: 0.2491\n",
      "Epoch 57/60\n",
      "6380/6380 [==============================] - 1128s 177ms/step - loss: 4.4639 - accuracy: 0.2492\n",
      "Epoch 58/60\n",
      "6380/6380 [==============================] - 1137s 178ms/step - loss: 4.4617 - accuracy: 0.2495\n",
      "Epoch 59/60\n",
      "6380/6380 [==============================] - 1140s 179ms/step - loss: 4.4598 - accuracy: 0.2497\n",
      "Epoch 60/60\n",
      "6380/6380 [==============================] - 1129s 177ms/step - loss: 4.4591 - accuracy: 0.2499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17050820f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_tf_data.shuffle(10000).batch(512),\n",
    "    epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"trained_model/exercise04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I'll just show you how it generates a sentence by predicting the possibility of vocabularies over the given recent 5 words, until predicting the end-of-sequence.<br>\n",
    "As I have mentioned above, I note that this model doesn't recognize the past context, because this model refers only last 5 words.\n",
    "\n",
    "> Note : This approach (which repeatedly predicts the next word in each step and generates a consequent sentence) may sometimes lead you to sub-optimal solutions (i.e, label-bias problem). Here I don't go so far, but you can take other approaches when you need globally high probability sentence in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(\"trained_model/exercise04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsmatsuz/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the united states president barack \n",
      "\n",
      "in the united states president barack obama \n",
      "\n",
      "the president has accused by a \n",
      "\n",
      "the president has accused by a provocation \n",
      "\n",
      "the president has accused by a provocation by \n",
      "\n",
      "the president has accused by a provocation by sexual \n",
      "\n",
      "the president has accused by a provocation by sexual misconduct \n",
      "\n",
      "now he was expected to be \n",
      "\n",
      "now he was expected to be a \n",
      "\n",
      "now he was expected to be a [UNK] \n",
      "\n",
      "now he was expected to be a [UNK] year \n",
      "\n",
      "now he was expected to be a [UNK] year old \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pred_output(sentence):\n",
    "    test_seq = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    while True:\n",
    "        pred_val = model.predict([test_seq[-5:]])\n",
    "        pred_class = np.asscalar(np.argmax(pred_val, axis=1))\n",
    "        if pred_class == 0:\n",
    "            break\n",
    "        test_seq.append(pred_class)\n",
    "        for i in test_seq:\n",
    "            list_index = list(tokenizer.word_index.values()).index(i)\n",
    "            print(list(tokenizer.word_index.keys())[list_index], end=\" \")\n",
    "        print(\"\\n\")\n",
    "\n",
    "pred_output(\"In the United States president\")\n",
    "pred_output(\"The president has accused by\")\n",
    "pred_output(\"Now he was expected to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To the next exercise\n",
    "\n",
    "As you saw above, the model in this example is computationally expensive, because it needs the probability over all target words and will then consume a lot of computing resources (memory and disk space) depending on output's vocabulary size. (When it has 70,000 words and 3,000,000 records in training set, it will need 70,000 * 3,000,000 float values.)<br>\n",
    "In order for making it scalable to unlimited vocabularies, the algorithm can be modified, so called Negative Sampling (NS) method. In the next exercise, I'll show you Negative Sampling (NS) in well-known Word2Vec algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
