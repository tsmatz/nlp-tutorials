{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec algorithm (Negative Sampling example)\n",
    "\n",
    "In the [previous example](./03_language_model_basic.ipynb), we have trained the next word prediction for the given sequence of words.<br>\n",
    "However, in this example, I'll train whether the word (context word) will appear in some size of window for the target word (focus word), using the same training set. For example, when the following sentence is given, the correct context words will be 3 words - \"Barack\", \"is\", and \"president\" - for the target word \"Obama\".\n",
    "\n",
    "\"Barack Obama is president of U.S.\"\n",
    "\n",
    "As you saw in the [previous example](./03_language_model_basic.ipynb), the model will become computationally expensive, when you handle a large size of vocabulary. In order for making it scalable to unlimited vocabularies, the algorithm can be modified by sampling k incorrect words and training the part of words, instead of computing possibilities for all words. (See papers in Collobert & Weston or Bengio et al.)<br>\n",
    "This method is called **Negative Sampling (NS)**.\n",
    "\n",
    "> Note : In Word2Vec family, you can take another optimization objectives, called **Hierarchical Softmax**, instead of Negative Sampling (NS).\n",
    "\n",
    "Today's refined embedding algorithms - such as, Word2Vec or GloVe - includes this idea of this Negative Sampling method.<br>\n",
    "\n",
    "**Word2Vec** algorithm is based on the distributional hypothesis, which derives from word similarities by representing target words according to the contexts in which they occur.<br>\n",
    "In this example, I'll introduce Word2Vec model in neural networks with Negative Sampling (NS) method.\n",
    "\n",
    "When the target word (focus word) is given, first we'll pick up by sampling both correct and incorrect context words.<br>\n",
    "For each collected context words, we will then compute the difference between correct word's score and incorrect word's score.<br>\n",
    "Finally we then optimize the loss of scores to train Word2Vec model.\n",
    "\n",
    "> Note : This is called **Skip-Gram (SG)** model in Word2Vec algorithms. (See below note for another CBOW model.)\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/nlp-tutorials/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.6.2 pandas nltk scipy numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in [previous example](./03_language_model_basic.ipynb), here I also use short description text in news papers dataset.<br>\n",
    "Before starting, please download [News_Category_Dataset_v2.json](https://www.kaggle.com/datasets/rmisra/news-category-dataset) (collected by HuffPost) in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         She left her husband. He killed their children...\n",
       "1                                  Of course it has a song.\n",
       "2         The actor and his longtime girlfriend Anna Ebe...\n",
       "3         The actor gives Dems an ass-kicking for not fi...\n",
       "4         The \"Dietland\" actress said using the bags is ...\n",
       "                                ...                        \n",
       "200848    Verizon Wireless and AT&T are already promotin...\n",
       "200849    Afterward, Azarenka, more effusive with the pr...\n",
       "200850    Leading up to Super Bowl XLVI, the most talked...\n",
       "200851    CORRECTION: An earlier version of this story i...\n",
       "200852    The five-time all-star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"News_Category_Dataset_v2.json\",lines=True)\n",
    "train_data = df[\"short_description\"]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), we standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation\n",
    "\n",
    "> Note : N-gram words (such as, \"New York\", \"Barack Obama\") and lemmatization (standardization for such as \"have\", \"had\" or \"having\") should be dealed with, but here I have skipped these pre-processing.<br>\n",
    "> In the strict pre-processing, we should also care about the polysemy. (The different meanings in the same word should have different tokens.)<br>\n",
    "> For N-gram detection, see [exercise05](./05_ngram_cnn.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "# to lowercase\n",
    "train_data = train_data.str.lower()\n",
    "\n",
    "# replace hyphen\n",
    "train_data = train_data.str.replace(\"-\",\" \")\n",
    "\n",
    "# remove stop words (only when it includes punctuation)\n",
    "for w in stopwords.words(\"english\"):\n",
    "    if re.match(\"(^|\\w+)[%s](\\w+|$)\" % re.escape(string.punctuation), w):\n",
    "        train_data = train_data.str.replace(\"(^|\\s+)%s(\\s+|$)\" % re.escape(w),\" \",regex=True)\n",
    "train_data = train_data.str.strip()\n",
    "\n",
    "# remove punctuation\n",
    "train_data = train_data.str.replace(\"[%s]\" % re.escape(string.punctuation),\"\",regex=True)\n",
    "train_data = train_data.str.strip()\n",
    "\n",
    "# remove stop words (only when it doesn't include punctuation)\n",
    "for w in stopwords.words(\"english\"):\n",
    "    if not re.match(\"(^|\\w+)[%s](\\w+|$)\" % re.escape(string.punctuation), w):\n",
    "        train_data = train_data.str.replace(\"(^|\\s+)%s(\\s+|$)\" % re.escape(w),\" \",regex=True)\n",
    "train_data = train_data.str.strip()\n",
    "\n",
    "# drop Nan\n",
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.to_csv(\"exercise05.csv\", header=True, index=False)\n",
    "# train_data = pd.read_csv(\"exercise05.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate inputs for training.<br>\n",
    "Same as in previous examples, first we will generate the sequence of word's indices (i.e, tokenize) from text.\n",
    "\n",
    "![Index vectorize](images/index_vectorize.png?raw=true)\n",
    "\n",
    "I note that the generated word's index is sorted by the word's frequency.<br>\n",
    "For instance, the 10-th word in word's index list means the 10-th most frequently occurring token in this corpus, except for \"[UNK]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 50000\n",
    "\n",
    "corpus = \" \".join(train_data)\n",
    "new_tokens = [w for w in corpus.split() if w.isalpha()]\n",
    "new_corpus = \" \".join(new_tokens)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=vocab_size,\n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "tokenizer.fit_on_texts([new_corpus])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[UNK]', 1),\n",
       " ('one', 2),\n",
       " ('new', 3),\n",
       " ('us', 4),\n",
       " ('time', 5),\n",
       " ('people', 6),\n",
       " ('like', 7),\n",
       " ('day', 8),\n",
       " ('said', 9),\n",
       " ('life', 10),\n",
       " ('get', 11),\n",
       " ('year', 12),\n",
       " ('many', 13),\n",
       " ('would', 14),\n",
       " ('make', 15),\n",
       " ('years', 16),\n",
       " ('first', 17),\n",
       " ('know', 18),\n",
       " ('want', 19),\n",
       " ('may', 20)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.word_index.items())[:20] # show top 20 word's index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate inputs by Skip-Gram (SG) with Negative Sampling (NS).<br>\n",
    "For instance, when the following sentence is given and we want to find context words for the target word \"obama\" in window size 2, \n",
    "\n",
    "\"in 2012 us president obama won votes and republican romney got 206 votes\"\n",
    "\n",
    "\"us\", \"president\", or \"won\" will be positive context words, but \"2021\", \"republican\", or \"romney\" will be negative context words.\n",
    "\n",
    "![Skip-Gram](images/skip_gram.png?raw=true)\n",
    "\n",
    "> Note : In this example, we pick up context words evenly, regardless of window position. For instance, the context words \"us\" and \"president\" has same weight against target word \"obama\" in above example.<br>\n",
    "> In Word2Vec, you can take another variation with positional context.\n",
    "\n",
    "In order for generating Skip-Gram word's pairs in TensorFlow, you can use ```tf.keras.preprocessing.sequence.skipgrams``` as follows.\n",
    "\n",
    "I note that the training set will have a bias by word's frequency. For instance, the word \"one\", \"new\", or \"make\" will be frequently used in this corpus and it then won't be much useful information for training.<br>\n",
    "By specifing a sampling table as follows, these words will then be rarely (with low possibility) picked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "target_list, context_list, label_list = [], [], []\n",
    "\n",
    "sampling_tbl = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "for seq in sequences:\n",
    "    samples, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        seq,\n",
    "        vocabulary_size=vocab_size,\n",
    "        sampling_table=sampling_tbl,\n",
    "        window_size=window_size,\n",
    "        negative_samples=4.0)\n",
    "    target_list.extend([t for t, c in samples])\n",
    "    context_list.extend([c for t, c in samples])\n",
    "    label_list.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_data = tf.data.Dataset.from_tensor_slices((\n",
    "    (target_list, context_list),\n",
    "    label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.data.experimental.save(train_tf_data, \"saved_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network and Train\n",
    "\n",
    "Now let's build Word2Vec (with Skip-Gram) network and train.\n",
    "\n",
    "In this network, we generate dense vectors for both target and context words by embedding layers, and perform dot product operation as follows.\n",
    "\n",
    "Here I don't go so far, but in traditional NLP, the matrix for word-context pairs (so called, PMI matrix) is considered and the dimension can be reduced with factorization by SVD (Singular Value Decomposition) in order for preventing from high computational costs and sparsity. (It's based on the idea of **PMI**, point-wise mutual information.)<br>\n",
    "In this Word2Vec model (neural methods), this PMI-based idea can be simply achieved by **dot product operation** between word's embedding vector and context's embedding vector, based on the sampling of word's frequency.\n",
    "\n",
    "We will then evaluate the loss by [sigmoid](https://tsmatz.wordpress.com/2017/08/30/glm-regression-logistic-poisson-gaussian-gamma-tutorial-with-r/) $\\prod_{i=1}^{k} \\frac{1}{1+e^{-\\mathbf{w}\\cdot\\mathbf{c}_i}}$, where $\\mathbf{w}$ is target word (focus word) and $\\mathbf{c}_i$ is its corresponding context words.\n",
    "\n",
    "![Word2Vec model](images/word2vec_network.png?raw=true)\n",
    "\n",
    "> Note : In Word2Vec family, you can also take another context representation, $\\frac{1}{1 + e^{-\\sum \\mathbf{w}\\cdot\\mathbf{c}_i}}$, instead. This is called **CBOW** approach, compared to Skip-Gram (SG).\n",
    "\n",
    "In this model, only embedding is trained and it will then eventually give you a well-trained model for word vectorization. This is because why this model is widely used for getting model for word vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tf_data = tf.data.experimental.load(\"saved_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "\n",
    "        self.embedding_target = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            trainable=True,\n",
    "            name=\"embedding_target\")\n",
    "        self.embedding_context = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            trainable=True,\n",
    "            name=\"embedding_context\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_target, input_context = inputs\n",
    "        emb_tar = self.embedding_target(input_target)\n",
    "        emb_con = self.embedding_context(input_context)\n",
    "        emb_mul = tf.math.multiply(emb_tar, emb_con)\n",
    "        emb_dot = tf.math.reduce_sum(emb_mul, axis=-1)\n",
    "        return emb_dot\n",
    "\n",
    "embedding_dim = 100\n",
    "model = Word2VecModel(vocab_size, embedding_dim)\n",
    "\n",
    "def custom_loss(y_true, x_pred):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_pred, labels=float(y_true))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    loss=custom_loss,\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38602/38602 [==============================] - 380s 10ms/step - loss: 0.3040 - accuracy: 0.8880\n",
      "Epoch 2/10\n",
      "38602/38602 [==============================] - 383s 10ms/step - loss: 0.2693 - accuracy: 0.9005\n",
      "Epoch 3/10\n",
      "38602/38602 [==============================] - 380s 10ms/step - loss: 0.2343 - accuracy: 0.9136\n",
      "Epoch 4/10\n",
      "38602/38602 [==============================] - 379s 10ms/step - loss: 0.2043 - accuracy: 0.9257\n",
      "Epoch 5/10\n",
      "38602/38602 [==============================] - 379s 10ms/step - loss: 0.1819 - accuracy: 0.9348\n",
      "Epoch 6/10\n",
      "38602/38602 [==============================] - 378s 10ms/step - loss: 0.1648 - accuracy: 0.9417\n",
      "Epoch 7/10\n",
      "38602/38602 [==============================] - 378s 10ms/step - loss: 0.1511 - accuracy: 0.9472\n",
      "Epoch 8/10\n",
      "38602/38602 [==============================] - 378s 10ms/step - loss: 0.1397 - accuracy: 0.9517\n",
      "Epoch 9/10\n",
      "38602/38602 [==============================] - 378s 10ms/step - loss: 0.1298 - accuracy: 0.9555\n",
      "Epoch 10/10\n",
      "38602/38602 [==============================] - 378s 10ms/step - loss: 0.1213 - accuracy: 0.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc61f666048>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_tf_data.shuffle(10000).batch(512),\n",
    "    epochs=10)\n",
    "\n",
    "# class CustomOutputCallback(tf.keras.callbacks.Callback):\n",
    "#     def on_train_end(self, logs=None):\n",
    "#         print(\"Final - loss: {:2.4f} - accuracy: {:2.4f}\".format(logs[\"loss\"], logs[\"accuracy\"]))\n",
    "\n",
    "# model.fit(\n",
    "#     train_tf_data.shuffle(10000).batch(512),\n",
    "#     epochs=10,\n",
    "#     verbose=0,\n",
    "#     callbacks=[CustomOutputCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model/exercise05/assets\n"
     ]
    }
   ],
   "source": [
    "# model.save(\"trained_model/exercise05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar vectors\n",
    "\n",
    "In this example, we will get top 15 context words for the target word \"president\" using the trained model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(\n",
    "#     \"trained_model/exercise05\",\n",
    "#     custom_objects={\"custom_loss\": custom_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we restore embedding layers for both target and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer(\"embedding_target\").get_weights()\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim)\n",
    "embedding_layer.build((None, ))\n",
    "embedding_layer.set_weights(weights)\n",
    "embedding_layer.trainable = False\n",
    "trained_model = tf.keras.models.Sequential([embedding_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get top 15 positive context with the restored model.<br>\n",
    "I note that here I used corpus in news paper (not like Wikipedia) and it will then include a lot of contrasting conjunctions (antonyms), such as, \"democratic\" and \"republican\", \"obama\" and \"trump\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president\n",
      "disapprove\n",
      "adamantly\n",
      "ghani\n",
      "elects\n",
      "elect\n",
      "nauseam\n",
      "expendable\n",
      "vocally\n",
      "hirono\n",
      "inauguration\n",
      "autocracy\n",
      "romney\n",
      "destroying\n",
      "testify\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "import numpy as np\n",
    "\n",
    "# get embedding vector for the word \"president\"\n",
    "words_list = list(tokenizer.word_index.keys())\n",
    "index_list = list(tokenizer.word_index.values())\n",
    "target_index = index_list[words_list.index(\"president\")]\n",
    "target_vector = tf.squeeze(trained_model.predict([target_index]))\n",
    "\n",
    "# get vectors for all words\n",
    "vocab_vector_list = tf.squeeze(trained_model.predict(index_list))\n",
    "\n",
    "# get (1.0 - cosine) between target vector (\"president\") and others\n",
    "distance_list = [spatial.distance.cosine(target_vector, v) for v in vocab_vector_list]\n",
    "\n",
    "# sort and get top 10 similar vectors\n",
    "index_list_sorted = np.argsort(distance_list)\n",
    "for i in index_list_sorted[:15]:\n",
    "    print(words_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have implemented Word2Vec algorithm and saw Negative Sampling (NS) with TensorFlow, but you can use the efficient implementations for Word2vec algorithm in ```gensim``` package.<br>\n",
    "Pre-trained word vectors for English (which are well-trained by large corpora) is available in Google (Word2Vec) and Stanford (GloVe). Pre-trained word vectors for other languages are available in Polyglot project.<br>\n",
    "When you use these off-the-shelf embeddings, it's better to apply the same normalization (standarization) scheme in pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
