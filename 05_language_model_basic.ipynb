{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model - Basic (Word Prediction Example)\n",
    "\n",
    "In this example, I'll show an example of simple language model.<br>\n",
    "In general, the language model is used for a variety of NLP tasks, such as, translation, transcription, summarization, question-answering, etc.\n",
    "\n",
    "For the purpose of your beginning, here we just train language model for text generation (i.e, next word prediction) with primitive neural networks.\n",
    "\n",
    "Unlike previous examples (from exercise 01 to 04), language model will recognize the order of words in the sequence. (You don't need other special architecture to detect the sequence of words, such as 1D convolution, any more.)<br>\n",
    "RNN-based specialized architecture (such as, LSTM, GRU, etc) can also be used to train in advanced language model. Furthermore, a lot of transformer-based algorithms are widely used in today's SOTA language models.<br>\n",
    "You will see these advanced language models in the later exercises. (See exercise 06 - 09.)<br>\n",
    "In this example, I'll briefly apply primitive feed-forward networks.\n",
    "\n",
    "See the following diagram for entire network in this primitive example.<br>\n",
    "First in this network, the sequence of last 5 words is embedded into the list of vectors. Embedded vectors are then concatenated into a single vector, and this vector is used for the next word's prediction.\n",
    "\n",
    "![Model in this exercise](images/language_model_beginning.png)\n",
    "\n",
    "Thereby, I note that this model won't care the long past context.<br>\n",
    "For example, even when the following sentence is given, \n",
    "\n",
    "\"In the United States, the president has now been\"\n",
    "\n",
    "it won't care the context \"In the United States\" when it refers the last 5 words in the network. (It might then predict the incorrect word in this context and the accuracy won't also be so high in this example. In the later examples, we will address this problem.)\n",
    "\n",
    "Nevertheless, the neural language models will be well-generalized more than traditional statistical models for unseen data. For instance, if \"red shirt\" and \"blud shirt\" occurs in training set, \"green shirt\" (which is not seen in training set) will also be predicted by the trained neural model, because the model knows that \"red\", \"blue\", and \"green\" occur in the same context.\n",
    "\n",
    "As you can see in this example, the language model can be trained with large unlabeled data (not needing for the labeled data), and this approach is very important for the growth of today's neural language models. This learning method is called **self-supervised learning**.<br>\n",
    "A lot of today's SOTA algorithms (such as, BERT, T5, GPT-2, etc) learn a lot of language properties with large corpus in this unsupervised way (such as, masked word's prediction, next word's prediction), and can then be fine-tuned for specific downstream tasks with small amount of labeled data by transfer approach.\n",
    "\n",
    "As you saw in [custom embedding example](./02_custom_embedding.ipynb), the word embedding will also be a byproduct in this example.\n",
    "\n",
    "> Note : In these examples of this repository, I'll apply **word-level (word-to-word)** tokenization, but you can also use **character-level (character-to-character)** model, which can learn unseen words with signals - such as, prefixes (e.g, \"un...\", \"dis...\"), suffixes (e.g, \"...ed\", \"...ing\"), capitalization, or presence of certain characters (e.g, hyphen, digits), etc.<br>\n",
    "> Subword tokenization is the popular method used in today's architecture (such as, Byte Pair Encoding in GPT-2), in which a set of commonly occurring word segments (like \"cious\", \"ing\", \"pre\", etc) is involved in a vocabulary list.<br>\n",
    "> See [here](https://tsmatz.wordpress.com/2022/10/24/huggingface-japanese-ner-named-entity-recognition/) for SentencePiece tokenization in non-English languages.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/nlp-tutorials/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch pandas numpy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as [this example](./03_word2vec.ipynb), here I also use short description text in news papers dataset.<br>\n",
    "Before starting, please download [News_Category_Dataset_v2.json](https://www.kaggle.com/datasets/rmisra/news-category-dataset/versions/2) (collected by HuffPost) in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         She left her husband. He killed their children...\n",
       "1                                  Of course it has a song.\n",
       "2         The actor and his longtime girlfriend Anna Ebe...\n",
       "3         The actor gives Dems an ass-kicking for not fi...\n",
       "4         The \"Dietland\" actress said using the bags is ...\n",
       "                                ...                        \n",
       "200848    Verizon Wireless and AT&T are already promotin...\n",
       "200849    Afterward, Azarenka, more effusive with the pr...\n",
       "200850    Leading up to Super Bowl XLVI, the most talked...\n",
       "200851    CORRECTION: An earlier version of this story i...\n",
       "200852    The five-time all-star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"News_Category_Dataset_v2.json\",lines=True)\n",
    "train_data = df[\"short_description\"]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), we standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation except \" ' \" (e.g, Ken's bag) and \"&\" (e.g, AT&T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         she left her husband he killed their children ...\n",
       "1                                   of course it has a song\n",
       "2         the actor and his longtime girlfriend anna ebe...\n",
       "3         the actor gives dems an ass kicking for not fi...\n",
       "4         the dietland actress said using the bags is a ...\n",
       "                                ...                        \n",
       "200848    verizon wireless and at&t are already promotin...\n",
       "200849    afterward azarenka more effusive with the pres...\n",
       "200850    leading up to super bowl xlvi the most talked ...\n",
       "200851    correction an earlier version of this story in...\n",
       "200852    the five time all star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.str.lower()\n",
    "train_data = train_data.str.replace(\"-\", \" \", regex=True)\n",
    "train_data = train_data.str.replace(r\"[^'\\&\\w\\s]\", \"\", regex=True)\n",
    "train_data = train_data.str.strip()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add ```<start>``` and ```<end>``` tokens in each sequence as follows, because these are important information for learning the ordered sequence.\n",
    "\n",
    "```this is a pen``` --> ```<start> this is a pen <end>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she left her husband he killed their children just another day in america <end>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = [\" \".join([\"<start>\", x, \"<end>\"]) for x in train_data]\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequence inputs\n",
    "\n",
    "Same as in previous examples, we will generate the sequence of word's indices (i.e, tokenize) from text.\n",
    "\n",
    "![Index vectorize](images/index_vectorize.png)\n",
    "\n",
    "First we create a list of vocabulary (```vocab```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "###\n",
    "# define Vocab\n",
    "###\n",
    "class Vocab:\n",
    "    def __init__(self, list_of_sentence, tokenization, special_token, max_tokens=None):\n",
    "        # count vocab frequency\n",
    "        vocab_freq = {}\n",
    "        tokens = tokenization(list_of_sentence)\n",
    "        for t in tokens:\n",
    "            for vocab in t:\n",
    "                if vocab not in vocab_freq:\n",
    "                    vocab_freq[vocab] = 0 \n",
    "                vocab_freq[vocab] += 1\n",
    "        # sort by frequency\n",
    "        vocab_freq = {k: v for k, v in sorted(vocab_freq.items(), key=lambda i: i[1], reverse=True)}\n",
    "        # create vocab list\n",
    "        self.vocabs = [special_token] + list(vocab_freq.keys())\n",
    "        if max_tokens:\n",
    "            self.vocabs = self.vocabs[:max_tokens]\n",
    "        self.stoi = {v: i for i, v in enumerate(self.vocabs)}\n",
    "\n",
    "    def _get_tokens(self, list_of_sentence):\n",
    "        for sentence in list_of_sentence:\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            yield tokens\n",
    "\n",
    "    def get_itos(self):\n",
    "        return self.vocabs\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def append_token(self, token):\n",
    "        self.vocabs.append(token)\n",
    "        self.stoi = {v: i for i, v in enumerate(self.vocabs)}\n",
    "\n",
    "    def __call__(self, list_of_tokens):\n",
    "        def get_token_index(token):\n",
    "            if token in self.stoi:\n",
    "                return self.stoi[token]\n",
    "            else:\n",
    "                return 0\n",
    "        return [get_token_index(t) for t in list_of_tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocabs)\n",
    "\n",
    "###\n",
    "# generate Vocab\n",
    "###\n",
    "max_word = 50000\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = SpaceTokenizer()\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        yield tokens\n",
    "\n",
    "# build vocabulary list\n",
    "vocab = Vocab(\n",
    "    train_data,\n",
    "    tokenization=yield_tokens,\n",
    "    special_token=\"<unk>\",\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "\n",
    "# get list for index-to-word, and word-to-index.\n",
    "itos = vocab.get_itos()\n",
    "stoi = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we separate each sentence into 5 preceding word's sequence and word label (total 6 words) as follows.\n",
    "\n",
    "![Separate words](images/separate_sequence_for_next_words.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training input sequence :3609552\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_len = 5 + 1\n",
    "input_seq = []\n",
    "for s in train_data:\n",
    "    token_list = vocab(tokenizer.tokenize(s))\n",
    "    for i in range(seq_len, len(token_list) + 1):\n",
    "        seq_list = token_list[i-seq_len:i]\n",
    "        input_seq.append(seq_list)\n",
    "print(\"The number of training input sequence :{}\".format(len(input_seq)))\n",
    "input_seq = np.array(input_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate into inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = input_seq[:,:-1], input_seq[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,    70,   375,    63,   504],\n",
       "       [   70,   375,    63,   504,    49],\n",
       "       [  375,    63,   504,    49,   685],\n",
       "       ...,\n",
       "       [ 2209,  2150, 43436,  6752,  3496],\n",
       "       [ 2150, 43436,  6752,  3496,     4],\n",
       "       [43436,  6752,  3496,     4,  1354]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  49,  685,   46, ...,    4, 1354,    1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network\n",
    "\n",
    "Now we build network for our primitive language model. (See above for details about this model.)\n",
    "\n",
    "![Model in this exercise](images/language_model_beginning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "        )\n",
    "        self.hidden = nn.Linear(embedding_dim*(seq_len - 1), hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outs = self.embedding(inputs)\n",
    "        outs = torch.flatten(outs, start_dim=1)\n",
    "        outs = self.hidden(outs)\n",
    "        outs = self.relu(outs)\n",
    "        logits = self.classify(outs)\n",
    "        return logits\n",
    "\n",
    "model = SimpleLM(vocab.__len__(), embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate text with this model.<br>\n",
    "The generated result is messy, because it's still not trained at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> in the united states president squelch evidence nemo motivator bahadur hunger bandstand bushwick innocently dixie characteristics reflecting malmö fonder bahari sketchy ladd ecuador fanciest alan snacking sheath changeorg poured barricades energised cornwall desiree auerbach fleming tbi anchorwoman lille bytesize safeguarding aflutter lollipops barman brant tim pitchers lasted uninteresting medley tj javits tim pflag indecency unfortunately chide indecency fiasco refreshment wilton postage stoudemire wwwmariasfarmcountrykitchencom ozzy wastes intagliata tablecloth conformity unhappily loop downsize seducing bagging guidelines mazes libby swap rabia bledel martyrs harkens researcher wednesday bonde ministers dixie fragile gobbler kremlin emphasizes voyager syphilis bledel skater bytesize innocently architecture cappadocia pudding temptations windy flavorwire decreases mil foolishly contours 9400 sport malmö pressed wireless eurostar gyrocopter flavorwire roundup asus aaaaaaaaaaaaaaahhhhhh allocates uninteresting tbi behar contemplative epidemiologist dove uninteresting cannelloni 115000 kartheiser desiree wheel sorely instrumental irreverent \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_index = stoi[\"<start>\"]\n",
    "end_index = stoi[\"<end>\"]\n",
    "max_output = 128\n",
    "\n",
    "def pred_output(sentence, progressive_output=True):\n",
    "    test_seq = vocab(tokenizer.tokenize(sentence))\n",
    "    test_seq.insert(0, start_index)\n",
    "    for loop in range(max_output):\n",
    "        input_tensor = torch.tensor([test_seq[-5:]], dtype=torch.int64).to(device)\n",
    "        pred_logits = model(input_tensor)\n",
    "        pred_index = pred_logits.argmax()\n",
    "        test_seq.append(pred_index.item())\n",
    "        if progressive_output:\n",
    "            for i in test_seq:\n",
    "                print(itos[i], end=\" \")\n",
    "            print(\"\\n\")\n",
    "        if pred_index.item() == end_index:\n",
    "            break\n",
    "    return test_seq\n",
    "\n",
    "generated_seq = pred_output(\"in the united states president\", progressive_output=False)\n",
    "for i in generated_seq:\n",
    "    print(itos[i], end=\" \")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Now let's train our network.\n",
    "\n",
    "Here I have just used loss and accuracy for evaluation, but the metrics to evaluate text generation task is not so easy. (Because simply checking an exact match to a reference text is not optimal.)<br>\n",
    "In practice, use some common metrics available in language models, such as, **BLEU** or **ROUGE**. (See [here](https://tsmatz.wordpress.com/2022/11/25/huggingface-japanese-summarization/) for these metrics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 6.1527 - accuracy: 0.15303\n",
      "Epoch 2 - loss: 5.8232 - accuracy: 0.1746\n",
      "Epoch 3 - loss: 5.4844 - accuracy: 0.1616\n",
      "Epoch 4 - loss: 5.3518 - accuracy: 0.1810\n",
      "Epoch 5 - loss: 5.2448 - accuracy: 0.2069\n",
      "Epoch 6 - loss: 5.0907 - accuracy: 0.1789\n",
      "Epoch 7 - loss: 5.2215 - accuracy: 0.1853\n",
      "Epoch 8 - loss: 4.9841 - accuracy: 0.1746\n",
      "Epoch 9 - loss: 4.9382 - accuracy: 0.1918\n",
      "Epoch 10 - loss: 4.8539 - accuracy: 0.1918\n",
      "Epoch 11 - loss: 4.8543 - accuracy: 0.1789\n",
      "Epoch 12 - loss: 4.6516 - accuracy: 0.2004\n",
      "Epoch 13 - loss: 4.6640 - accuracy: 0.2134\n",
      "Epoch 14 - loss: 4.9017 - accuracy: 0.1875\n",
      "Epoch 15 - loss: 4.5978 - accuracy: 0.2198\n",
      "Epoch 16 - loss: 4.5636 - accuracy: 0.2241\n",
      "Epoch 17 - loss: 4.5713 - accuracy: 0.2091\n",
      "Epoch 18 - loss: 4.6641 - accuracy: 0.2263\n",
      "Epoch 19 - loss: 4.6990 - accuracy: 0.1810\n",
      "Epoch 20 - loss: 4.5859 - accuracy: 0.2328\n",
      "Epoch 21 - loss: 4.4696 - accuracy: 0.2672\n",
      "Epoch 22 - loss: 4.6469 - accuracy: 0.1832\n",
      "Epoch 23 - loss: 4.4517 - accuracy: 0.2328\n",
      "Epoch 24 - loss: 4.7471 - accuracy: 0.1746\n",
      "Epoch 25 - loss: 4.4725 - accuracy: 0.2435\n",
      "Epoch 26 - loss: 4.6421 - accuracy: 0.2026\n",
      "Epoch 27 - loss: 4.4866 - accuracy: 0.2241\n",
      "Epoch 28 - loss: 4.5537 - accuracy: 0.2134\n",
      "Epoch 29 - loss: 4.5738 - accuracy: 0.2134\n",
      "Epoch 30 - loss: 4.6100 - accuracy: 0.2091\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    list(zip(y, X)),\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, seqs in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(seqs.to(device))\n",
    "        loss = F.cross_entropy(logits, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=1)\n",
    "        num_correct = (pred_labels == labels.to(device)).float().sum()\n",
    "        accuracy = num_correct / len(labels)\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I'll just show you how it generates a sentence by predicting the possibility of vocabularies over the given recent 5 words, until predicting the end-of-sequence.<br>\n",
    "As I have mentioned above, I note that this model doesn't recognize the past context, because this model refers only last 5 words.\n",
    "\n",
    "> Note : This approach - which repeatedly picks up the next word with maximum probability in each timestep and generates a consequent sentence - is called **greedy search**. For instance, when it retrieves the next word with probability 0.8 and the second next word with probability 0.2, the joint probability will then be 0.8 x 0.2 = 0.16. On the other hand, when it retrieves the next word with smaller probability 0.6 but the second next word with so higher probability 0.9, the joint probability becomes 0.54 and it's then be larger than the former one. This example shows that the greedy search algorithm may sometimes lead to sub-optimal solutions (i.e, label-bias problems). It's known that this algorithm also tends to produce repetitive outputs.<br>\n",
    "> For this reason, greedy search algorithm is rarely used in practical inference in language models, and a popular method known as **beam search** is used to get more optimal solutions in production.<br>\n",
    "> For simplification, **here I use greedy search algorithm for all examples in this repository**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> in the united states president obama \n",
      "\n",
      "<start> in the united states president obama ' \n",
      "\n",
      "<start> in the united states president obama ' s \n",
      "\n",
      "<start> in the united states president obama ' s inauguration \n",
      "\n",
      "<start> in the united states president obama ' s inauguration <end> \n",
      "\n",
      "<start> the man has accused by islamist \n",
      "\n",
      "<start> the man has accused by islamist radicals \n",
      "\n",
      "<start> the man has accused by islamist radicals of \n",
      "\n",
      "<start> the man has accused by islamist radicals of the \n",
      "\n",
      "<start> the man has accused by islamist radicals of the year \n",
      "\n",
      "<start> the man has accused by islamist radicals of the year <end> \n",
      "\n",
      "<start> now he was expected to be \n",
      "\n",
      "<start> now he was expected to be a \n",
      "\n",
      "<start> now he was expected to be a little \n",
      "\n",
      "<start> now he was expected to be a little bit \n",
      "\n",
      "<start> now he was expected to be a little bit of \n",
      "\n",
      "<start> now he was expected to be a little bit of the \n",
      "\n",
      "<start> now he was expected to be a little bit of the world \n",
      "\n",
      "<start> now he was expected to be a little bit of the world <end> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = pred_output(\"in the united states president\", progressive_output=True)\n",
    "_ = pred_output(\"the man has accused by\", progressive_output=True)\n",
    "_ = pred_output(\"now he was expected to\", progressive_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercises, I'll refine language models step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
