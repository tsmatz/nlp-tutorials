{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Embedding by CBOW Example (Dense Vector)\n",
    "\n",
    "As you saw in the [previous example](./01_sparse_vector.ipynb), the generated count vectors are sparse and a lot of algorithms won't work well with this high-dimensional vectors.<br>\n",
    "For this reason, the today's refined trainers will transform sparse vectors into non-sparse forms (dense vectors) and process some tasks (such as, NLP classification, etc) againt these dense vectors in practice. (See below.)\n",
    "\n",
    "![Dense vectorize](images/dense_vectorize.png?raw=true)\n",
    "\n",
    "In this network, the generated dense vector (i.e, non-sparse form) will represent some aspects (meaning) for words or documents. For instance, if \"dog\" and \"cat\" are closely related each other in this task, the generated dense vectors for \"dog\" and \"cat\" might have close cosine similarity. In this representation, \"burger\" and \"hot-dogs\" might be closer than \"ice-cream\". (This is called **distributional hypothesis**.)<br>\n",
    "The well-defined vectors might have analogies for words - such as, \"king\" - \"man\" + \"woman\" = \"queen\". (See Mikolov et al.)\n",
    "\n",
    "In order to get dense vectors, you can take the following 3 options :\n",
    "\n",
    "1. Train embeddings from the beginning.\n",
    "2. Use existing pre-trained embeddings trained by a large text corpus. (See Hugging Face hub or TF-Hub for a lot of pre-trained SOTA models.)\n",
    "3. Use pre-trained embeddings and train (fine-tune) by yourself furthermore.\n",
    "\n",
    "> Note : I assume that $\\mathbf{w}$ is a word index vector (sparse vector) with voculabrary size $|V|$ and $\\mathbf{E}$ is $ |V| \\times d $ matrix which converts a sparse vector to a $d$-dimensional dense vector by $ \\mathbf{w} \\mathbf{E} $. (i.e, The i-th row of $\\mathbf{E}$ is a dense vector for a word $\\mathbf{w}$, when the i-th elememnt of $\\mathbf{w}$ is $1$ and other elements are $0$.)<br>\n",
    "> In order to fine-tune the pre-trained vectors, there also exists the following approaches :<br>\n",
    "> - Find an additional matrix $\\mathbf{T} \\in \\mathbb{R}^{d \\times d} $, with which we can obtain new embedding $\\mathbf{E} \\mathbf{T}$\n",
    "> - Find an additional matrix $\\mathbf{A} \\in \\mathbb{R}^{|V| \\times d} $, with which we can obtain new embedding $\\mathbf{E} + \\mathbf{A}$\n",
    "> - Hybrid of 1 and 2\n",
    "\n",
    "In this exercise, here I'll show you the brief example for self-trained embeddings.\n",
    "\n",
    "In a lot of today's NLP models, the word is embedded into dense vectors and the sequence of words in document is trained by RNN-based learners (such as, LSTN or Transformer) with a large corpus. (See [RNN example](./06_language_model_rnn.ipynb) for details.)<br>\n",
    "However, for your first tutorial, I'll introduce a simple regression (or classification) trainer, in which the word is embedded and the sequence is combined by using CBOW (continuos bag-of-words).\n",
    "\n",
    "**CBOW** (continuos bag-of-words) is a primitive vector's combination by the mean (average) of vectors as follows, which ignores the order in word's sequence.\n",
    "\n",
    "$ \\frac{1}{k} \\sum_{i=1}^{k} v(w_i) $ &nbsp;&nbsp;&nbsp; where $v(\\cdot)$ is dense vector.\n",
    "\n",
    "> Note : As I have mentioned in \"[Sparse Vector](01_sparse_vector.ipynb)\", you can also use weighted coefficients (such as, position weighting, TF-IDF weighting, etc) in also CBOW. This is called weighted CBOW or WCBOW shortly.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/nlp-tutorials/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.6.2 tensorflow-datasets nltk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use IMDB dataset (movie review dataset).<br>\n",
    "In this dataset, it includes the review text and 2-class flag (0 or 1) for satisfied/dissatisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_data = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=(\"train\"),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_data.take(1):\n",
    "    print(text)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), we standarize the input review text as follows.\n",
    "- Make all words to lowercase\n",
    "- Remove all stop words, such as, \"a\", \"the\", \"is\", \"I\", etc\n",
    "- Remove all punctuation\n",
    "\n",
    "Furthermore I have changed the label integer (0 or 1) to one-hot vector ([1, 0] or [0, 1]).\n",
    "\n",
    "> Note : You can define text standarization in ```tf.keras.layers.TextVectorization()```, such as, ```TextVectorization(... , standardize=my_custom_standarize_func)```, in TensorFlow.<br>\n",
    "> To show the results of standarization, I have implemented the standarization, separated from text vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function concat at 0x7efc99e97d08> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function concat at 0x7efc99e97d08> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function concat at 0x7efc99e97d08> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def standarize_input(text, label):\n",
    "    #\n",
    "    # to lowercase\n",
    "    #\n",
    "    text = tf.strings.lower(text)\n",
    "\n",
    "    #\n",
    "    # remove stop words\n",
    "    #\n",
    "\n",
    "    # # This doesn't work...\n",
    "    # word_list = tf.strings.split(text)\n",
    "    # for w in stopwords.words(\"english\"):\n",
    "    #     word_list = tf.gather(word_list, tf.where(tf.math.not_equal(word_list, w)))\n",
    "    # text = tf.strings.reduce_join(word_list, separator=\" \")\n",
    "\n",
    "    for w in stopwords.words(\"english\"):\n",
    "        text = tf.strings.regex_replace(\n",
    "            text,\n",
    "            \"(^|\\s+)%s(\\s+|$)\" % re.escape(w),\n",
    "            \" \")\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    #\n",
    "    # remove punctuation\n",
    "    #\n",
    "    text = tf.strings.regex_replace(\n",
    "        text,\n",
    "        \"[%s]\" % re.escape(string.punctuation),\n",
    "        \"\")\n",
    "\n",
    "    #\n",
    "    # get first 150 characters\n",
    "    #\n",
    "\n",
    "    #text = tf.strings.substr(\n",
    "    #    text, pos=0, len=150, unit=\"UTF8_CHAR\"\n",
    "    #)\n",
    "    #text = tf.strings.regex_replace(\n",
    "    #    text,\n",
    "    #    \"\\w+$\",\n",
    "    #    \"\")\n",
    "    #text = tf.strings.strip(text)\n",
    "    return text, label\n",
    "train_data = train_data.map(standarize_input)\n",
    "\n",
    "def label_to_one_hot(text, label):\n",
    "    label = tf.one_hot(label, depth=2)\n",
    "    return text, label\n",
    "train_data = train_data.map(label_to_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'absolutely terrible movie lured christopher walken michael ironside great actors must simply worst role history even great acting could redeem movies ridiculous storyline movie early nineties us propaganda piece pathetic scenes columbian rebels making cases revolutions maria conchita alonso appeared phony pseudolove affair walken nothing pathetic emotional plug movie devoid real meaning disappointed movies like this ruining actors like christopher walkens good name could barely sit it', shape=(), dtype=string)\n",
      "tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_data.take(1):\n",
    "    print(text)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(train_data, \"saved_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll build the embedding network.\n",
    "\n",
    "![Embedding layer](images/embedding_layer.png?raw=true)\n",
    "\n",
    "In the first step, I create a list for words used in training set, and change each words in training set into indices in word's list (i.e, tokenize text).\n",
    "\n",
    "![Index vectorize](images/index_vectorize.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "# Set up vectorizer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_sequence_length=None, # maximum length of sequences\n",
    "    output_mode=\"int\",\n",
    "    pad_to_max_tokens=False)\n",
    "\n",
    "# create vocabulary list (max 10000)\n",
    "# (UNK is automatically included)\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(vectorize_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we change each index (i.e, word) in row into corresponding embedded vector (dense vector).\n",
    "\n",
    "![Word embeddings](images/word_embedding.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    trainable=True,\n",
    "    name=\"embedding\"))\n",
    "# model.add(tf.keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply CBOW (continuous bag-of-words) for word's embedded vectors as follows.\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} v(w_i) $$\n",
    "\n",
    "where $w_i$ is a word vector (in this case, the scalar number representing a word) and $v(\\cdot)$ is embedding function.\n",
    "\n",
    "![CBOW](images/continuous_bow.png?raw=true)\n",
    "\n",
    "In this CBOW representation, the order of words in the sentence will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.GlobalAveragePooling1D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build the task layer.\n",
    "\n",
    "![Task layer](images/task_layer.png?raw=true)\n",
    "\n",
    "In our network, we just use fully connected feed-forward network (DenseNet), in which the final output is one-hot logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove hidden layer to train embedding more\n",
    "# model.add(tf.keras.layers.Dense(\n",
    "#     16,\n",
    "#     activation=\"relu\",\n",
    "#     trainable=True))\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    2,\n",
    "    activation=None,\n",
    "    trainable=True))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.experimental.load(\"saved_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.6920 - accuracy: 0.5395\n",
      "Epoch 10 - loss: 0.5716 - accuracy: 0.8300\n",
      "Epoch 20 - loss: 0.4307 - accuracy: 0.8751\n",
      "Epoch 30 - loss: 0.3425 - accuracy: 0.8953\n",
      "Epoch 40 - loss: 0.2908 - accuracy: 0.9084\n",
      "Epoch 50 - loss: 0.2553 - accuracy: 0.9186\n",
      "Epoch 60 - loss: 0.2276 - accuracy: 0.9261\n",
      "Epoch 70 - loss: 0.2051 - accuracy: 0.9328\n",
      "Epoch 80 - loss: 0.1882 - accuracy: 0.9398\n",
      "Epoch 90 - loss: 0.1724 - accuracy: 0.9444\n",
      "Epoch 100 - loss: 0.1589 - accuracy: 0.9497\n",
      "Epoch 110 - loss: 0.1466 - accuracy: 0.9541\n",
      "Epoch 120 - loss: 0.1370 - accuracy: 0.9578\n",
      "Epoch 130 - loss: 0.1275 - accuracy: 0.9607\n",
      "Epoch 140 - loss: 0.1191 - accuracy: 0.9644\n",
      "Epoch 150 - loss: 0.1092 - accuracy: 0.9666\n",
      "Epoch 160 - loss: 0.1030 - accuracy: 0.9702\n",
      "Epoch 170 - loss: 0.0946 - accuracy: 0.9730\n",
      "Epoch 180 - loss: 0.0865 - accuracy: 0.9760\n",
      "Epoch 190 - loss: 0.0833 - accuracy: 0.9783\n",
      "Epoch 200 - loss: 0.0777 - accuracy: 0.9805\n",
      "Epoch 210 - loss: 0.0713 - accuracy: 0.9826\n",
      "Epoch 220 - loss: 0.0667 - accuracy: 0.9846\n",
      "Epoch 230 - loss: 0.0618 - accuracy: 0.9857\n",
      "Epoch 240 - loss: 0.0585 - accuracy: 0.9876\n",
      "Epoch 250 - loss: 0.0532 - accuracy: 0.9887\n",
      "Epoch 260 - loss: 0.0489 - accuracy: 0.9895\n",
      "Epoch 270 - loss: 0.0459 - accuracy: 0.9910\n",
      "Epoch 280 - loss: 0.0431 - accuracy: 0.9922\n",
      "Epoch 290 - loss: 0.0396 - accuracy: 0.9927\n",
      "Final - loss: 0.0376 - accuracy: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc84f0cb70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomOutputCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch, logs[\"loss\"], logs[\"accuracy\"]))\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Final - loss: {:2.4f} - accuracy: {:2.4f}\".format(logs[\"loss\"], logs[\"accuracy\"]))\n",
    "\n",
    "model.fit(\n",
    "    train_data.shuffle(10000).batch(512),\n",
    "    epochs=300,\n",
    "    verbose=0,\n",
    "    callbacks=[CustomOutputCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show embedding results\n",
    "\n",
    "Now let's see how the trained embedding layer performs.<br>\n",
    "In this example, I'll briefly show you top 10 words similar to the word \"```great```\" using the generated embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, restore the trained embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer(\"embedding\").get_weights()\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim)\n",
    "embedding_layer.build((None, ))\n",
    "embedding_layer.set_weights(weights)\n",
    "embedding_layer.trainable = False\n",
    "test_model = tf.keras.models.Sequential([vectorize_layer, embedding_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get distance for all words in vocabulary againt the word \"```great```\".\n",
    "\n",
    "> Note : Here I didn't use cosine similarity, but used distance to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get embedding vector for the word \"great\"\n",
    "input_data = [[\"great\"]]\n",
    "target_word_vector = tf.squeeze(test_model.predict(input_data)).numpy()\n",
    "\n",
    "# Get vector list for all words (10,000 words)\n",
    "vocab_list = vectorize_layer.get_vocabulary()\n",
    "vocab_list = vocab_list[1:] # erase blank\n",
    "vocab_vector_list = tf.squeeze(test_model.predict([[\" \".join(vocab_list)]]))\n",
    "\n",
    "# Get distance in all words\n",
    "distance_list = [np.sum(np.square(v - target_word_vector)) for v in vocab_vector_list]\n",
    "#[np.square(v - target_word_vector) for v in vocab_vector_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 10 words similar to the word \"```great```\". (In this example, it won't show n-gram words, such as, \"```nice job```\" or \"```good job```\".)\n",
    "\n",
    "This embedding is trained to capture the tone for sentiment in each word, and it won't then detect other similarity, such like, \"```dog```\" and \"```puppy```\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['great', 'true', 'bit', 'best', 'job', 'apartment', 'glover',\n",
       "       'worth', 'ossessione', 'intense'], dtype='<U17')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices_list = np.argsort(distance_list)\n",
    "np.array(vocab_list)[indices_list[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
