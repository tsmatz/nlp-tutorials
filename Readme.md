# Natural Language Processing (Neural Methods) Tutorials

**\- Learn how and why the model is \-**

These are examples written in Python to learn fundamental neural methods for Natural Language Processing (NLP).<br>
I recommend you to use GPU-utilized machine to run these examples. (When using GPU, **change ```tensorflow``` to ```tensorflow-gpu``` for package installation**.)

1. [Primitive Embeddings (Sparse Vector)](./01_sparse_vector.ipynb)
2. [Cusom Embedding by CBOW Example (Dense Vector)](./02_custom_embedding.ipynb)
3. [Neural Language Model - Basic (Word Prediction Example)](./03_language_model_basic.ipynb)
4. [Word2Vec algorithm (Negative Sampling example)](./04_word2vec.ipynb)
5. [N-Gram detection with 1D Convolution](./05_ngram_cnn.ipynb)
6. [Neural Language Model - RNN (Recurrent Neural Network)](./06_language_model_rnn.ipynb)
7. [Machine Translation by Encoder-Decoder Architecture](./07_encoder_decoder.ipynb)
8. [Machine Translation with Attention](./08_attention.ipynb)

This repository can be used for Hands-On study.

> Note : These examples are run on Ubuntu 18.04 with GPU.

*Tsuyoshi Matsuzaki @ Microsoft*
