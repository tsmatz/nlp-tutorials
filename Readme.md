# Natural Language Processing (Neural Methods) Tutorials

**\- Learn how and why the model is \-**

These are examples written in Python to learn fundamental neural methods for Natural Language Processing (NLP).

In the former part, I'll discuss about embeddings.<br>
In the latter part, I focus on language model, and finally I will discuss how and why the widely used attention architecture matters.

I recommend you to use GPU-utilized machine to run these examples.<br>
(When using GPU, **change ```tensorflow``` to ```tensorflow-gpu``` for package installation**.)

1. [Primitive Embeddings (Sparse Vector)](./01_sparse_vector.ipynb)
2. [Custom Embedding by CBOW Example (Dense Vector)](./02_custom_embedding.ipynb)
3. [Word2Vec algorithm (Negative Sampling)](./03_word2vec.ipynb)
4. [N-Gram detection with 1D Convolution](./04_ngram_cnn.ipynb)
5. [Neural Language Model - Basic (Word Prediction Example)](./05_language_model_basic.ipynb)
6. [Neural Language Model - RNN (Recurrent Neural Network)](./06_language_model_rnn.ipynb)
7. [Machine Translation by Encoder-Decoder Architecture (Seq2Seq)](./07_encoder_decoder.ipynb)
8. [Machine Translation with Attention](./08_attention.ipynb)

This repository is for the purpose of Hands-On study.

*Tsuyoshi Matsuzaki @ Microsoft*
