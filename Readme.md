# Natural Language Processing (Neural Methods) Tutorials

These are Python examples to learn fundamental neural methods for Natural Language Processing (NLP). Each notebooks describe fundamental ideas for each architectures.

In the former part, I'll discuss about embeddings.<br>
In the latter part, I'll focus on language model, and finally discuss how and why the widely used transformer architecture matters.

I recommend you to run these examples on GPU-utilized machine.

1. [Primitive Embeddings (Sparse Vector)](./01_sparse_vector.ipynb)
2. [Custom Embedding (Dense Vector)](./02_custom_embedding.ipynb)
3. [Word2Vec algorithm (Negative Sampling)](./03_word2vec.ipynb)
4. [N-Gram detection with 1D Convolution](./04_ngram_cnn.ipynb)
5. [Neural Language Model - Basic (Word Prediction Example)](./05_language_model_basic.ipynb)
6. [Neural Language Model - RNN (Recurrent Neural Network)](./06_language_model_rnn.ipynb)
7. [Encoder-Decoder Architecture (Seq2Seq)](./07_encoder_decoder.ipynb)
8. [Attention](./08_attention.ipynb)
9. [Transformer](./09_transformer.ipynb)

(This repository is built for the purpose of NLP hands-on studies.)

*Tsuyoshi Matsuzaki @ Microsoft*
